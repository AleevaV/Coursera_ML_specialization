{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, StratifiedKFold,train_test_split,cross_validate,GridSearchCV\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, scorer\n",
    "from sklearn.preprocessing import  OrdinalEncoder,OneHotEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_selection import SelectKBest,SelectFromModel\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Эксперименты с моделью\n",
    "\n",
    "На прошлой неделе вы поучаствовали в соревновании на kaggle и, наверняка, большинство успешно справилось с прохождением baseline, а значит пора двигаться дальше - заняться оптимизацией модели, провести серию экспериментов и построить сильное финальное решения.\n",
    "\n",
    "В этом задании вам нужно провести ряд эскпериментов, оценить качество полученных в процессе экспериментирования моделей и выбрать лучшее решение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет оцениваться на основании загруженного jupyther notebook и развернутых ответов на поставленные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инструкции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 231)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>819.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>z9ub4Lm</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5Acm</td>\n",
       "      <td>vJ_w8kB</td>\n",
       "      <td>WfJ2BB2SFSqauljlfOB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>v5hz20V</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>343.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20HE4Qn</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Xa3G</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>826.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4XQyovK</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>453m</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1641096.0</td>\n",
       "      <td>...</td>\n",
       "      <td>LTMqFbB</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4    Var5    Var6  Var7  Var8  Var9      Var10  ...  \\\n",
       "0   NaN   NaN   NaN   NaN     NaN   819.0   7.0   NaN   NaN        NaN  ...   \n",
       "1   NaN   NaN   NaN   NaN     NaN  2401.0  21.0   NaN   NaN        NaN  ...   \n",
       "2   NaN   NaN   NaN   NaN     NaN   343.0   0.0   NaN   NaN        NaN  ...   \n",
       "3   NaN   NaN   NaN   NaN     NaN   826.0   7.0   NaN   NaN        NaN  ...   \n",
       "4   NaN   NaN   NaN   NaN  3960.0     NaN   NaN   NaN   NaN  1641096.0  ...   \n",
       "\n",
       "    Var222      Var223  Var224  Var225  Var226   Var227               Var228  \\\n",
       "0  z9ub4Lm  LM8l689qOp     NaN     NaN    5Acm  vJ_w8kB  WfJ2BB2SFSqauljlfOB   \n",
       "1  v5hz20V  LM8l689qOp     NaN    kG3k    FSa2     RAYp              55YFVY9   \n",
       "2  20HE4Qn  LM8l689qOp     NaN     NaN    Xa3G     RAYp        F2FyR07IdsN7I   \n",
       "3  4XQyovK  LM8l689qOp     NaN    ELof    453m     RAYp        F2FyR07IdsN7I   \n",
       "4  LTMqFbB  LM8l689qOp     NaN     NaN    FSa2     RAYp        F2FyR07IdsN7I   \n",
       "\n",
       "   Var229  Var230  label  \n",
       "0     NaN     NaN     -1  \n",
       "1    mj86     NaN     -1  \n",
       "2     NaN     NaN     -1  \n",
       "3     NaN     NaN     -1  \n",
       "4     NaN     NaN     -1  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Data/train_dataset.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>Var11</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>819.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2401.0000</td>\n",
       "      <td>21.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>343.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>826.0000</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3960.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.641096e+06</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Var1    Var2    Var3    Var4       Var5       Var6     Var7    Var9  \\\n",
       "0  0.0001  0.0001  0.0001  0.0001     0.0001   819.0000   7.0000  0.0001   \n",
       "1  0.0001  0.0001  0.0001  0.0001     0.0001  2401.0000  21.0000  0.0001   \n",
       "2  0.0001  0.0001  0.0001  0.0001     0.0001   343.0000   0.0000  0.0001   \n",
       "3  0.0001  0.0001  0.0001  0.0001     0.0001   826.0000   7.0000  0.0001   \n",
       "4  0.0001  0.0001  0.0001  0.0001  3960.0000     0.0001   0.0001  0.0001   \n",
       "\n",
       "          Var10   Var11  ...  Var221  Var222  Var223  Var224  Var225  Var226  \\\n",
       "0  1.000000e-04  0.0001  ...     2.0  3344.0     0.0     1.0     2.0     2.0   \n",
       "1  1.000000e-04  0.0001  ...     4.0  3170.0     0.0     1.0     1.0     7.0   \n",
       "2  1.000000e-04  0.0001  ...     4.0    86.0     0.0     1.0     2.0    13.0   \n",
       "3  1.000000e-04  0.0001  ...     4.0   208.0     0.0     1.0     0.0     1.0   \n",
       "4  1.641096e+06  0.0001  ...     4.0  1156.0     0.0     1.0     2.0     7.0   \n",
       "\n",
       "   Var227  Var228  Var229  label  \n",
       "0     6.0    16.0     1.0     -1  \n",
       "1     2.0     2.0     2.0     -1  \n",
       "2     2.0     8.0     1.0     -1  \n",
       "3     2.0     8.0     1.0     -1  \n",
       "4     2.0     8.0     1.0     -1  \n",
       "\n",
       "[5 rows x 213 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted = []\n",
    "for var in data.columns:\n",
    "    if data[var].isna().sum() == data.shape[0]:\n",
    "        data.drop(var, axis=1,inplace=True)\n",
    "        deleted.append(var)\n",
    "# В категориальных признаках заменим пропуски на новую категорию\n",
    "data.loc[:,'Var191':] = data.loc[:,'Var191':].fillna('missing_value')\n",
    "np.array(deleted)\n",
    "\n",
    "data.loc[:, 'Var1':'Var190'] = data.loc[:, 'Var1':'Var190'].fillna(0.0001)\n",
    "\n",
    "cat_features = data.loc[:,'Var191':'Var229'].columns\n",
    "OE_enc = OrdinalEncoder()\n",
    "OE_enc.fit(data[cat_features])\n",
    "data[cat_features] = pd.DataFrame(OE_enc.transform(data[cat_features]), columns=cat_features)\n",
    "# data.label.str.replace(-1:'0')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:,-1]\n",
    "X = data.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Начнем с простого. Давайте оценим как много объектов действительно нужно для построения качественной модели. Для обучения доступна достаточно большая выборка и может так оказаться, что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. Постройте кривые обучения, обучая модель на выборках разного размера начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. Обратите внимание на `sklearn.model_selection.learning_curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стратегия кросс-валиадции - stratified K-Fold с 5 фолдами была выбрана ранее\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100,\n",
    "                                      max_features='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2400,  7800, 13200, 18600, 24000]),\n",
       " array([[0.97906072, 0.9879981 , 0.9879981 , 0.9879981 , 0.9879981 ],\n",
       "        [0.87302415, 0.87371961, 0.86857163, 0.87424239, 0.86857163],\n",
       "        [0.82956713, 0.83410902, 0.82890982, 0.83141866, 0.8288354 ],\n",
       "        [0.81187744, 0.81516479, 0.81107777, 0.81209328, 0.81556965],\n",
       "        [0.8035812 , 0.80242138, 0.79569194, 0.80664726, 0.80126758]]),\n",
       " array([[0.6646434 , 0.67882393, 0.67703998, 0.68240191, 0.67846887],\n",
       "        [0.69572146, 0.72071944, 0.71941883, 0.7134851 , 0.72428326],\n",
       "        [0.71697995, 0.72638739, 0.72730236, 0.72142671, 0.72530248],\n",
       "        [0.72107082, 0.72615362, 0.7344581 , 0.72787888, 0.73150488],\n",
       "        [0.71915551, 0.72998691, 0.7438719 , 0.73604667, 0.73121303]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc = learning_curve(gb_model, X, y, cv=cv, \n",
    "                    n_jobs=4, scoring=scorer.roc_auc_scorer)\n",
    "lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = lc[0]\n",
    "train_scores = lc[1].mean(axis=1)\n",
    "test_scores = lc[2].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgYAAAFNCAYAAAB7SKeSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeVxc5dn/8c8FBAgCCokQE2IgMUQDkUhQSoNZatW4pFZbW23q0mrz83m62Vat1vapXdJN69bNx9baWlNtfepabd2TmIpoQKghNSQGNCSGaMgCCWCA6/fHGSYzwxrgzIGZ6/16zSvMuc+cc/N1xrk5y3WLqmKMMcYYAxDjdQeMMcYYM3rYwMAYY4wxfjYwMMYYY4yfDQyMMcYY42cDA2OMMcb42cDAGGOMMX42MDDG+InIP0TkMq/7YYzxjlgdA2O8JyL1wJWq+pzXfTHGRDc7YmBMlBCROK/7MFyR8DsYM9rZwMCYUU5EzhWRKhHZIyIvi8iJAW3Xi8hbItIsIhtE5PyAtstF5F8icpuINAE3+ZatFZFbRGS3iNSJyFkBr1klIlcGvL6/dXNEZI1v38+JyK9E5P5+fo/zfL/HPl+fl/iW14vIRwPWu6l7OyKSLSIqIleIyDvACyLyTxH5Usi2q0XkAt/Px4vIsyLSJCIbReRTAeud7cupWUS2icg1Q/lvYkwks4GBMaOYiBQCvwf+HzAB+F/gcRFJ8K3yFnAqcCTwPeB+ETkmYBPFwBYgA1gRsGwjMBH4GXCPiEgfXehv3T8Dr/r6dRNwST+/xynAfcC1wFHAAqB+oN8/wELgBOBM334vDtj2bGAa8KSIHAE861snw7fer0Ukz7f6PcD/U9UUIB944TD6YExUsIGBMaPbF4D/VdVyVe1U1T8C7cCHAFT1IVXdrqpdqvoXYBNwSsDrt6vqL1S1Q1VbfcveVtXfqmon8EfgGCCzj/33uq6IHAucDPyPqn6gqmuBx/v5Pa4Afq+qz/r6uk1V3zyMHG5S1f2+3+ERYK6ITPO1LQMeVtV24FygXlXv9f3OlcDfgE/61j0IzBaRVFXd7Ws3xgSwgYExo9s04Bu+0wh7RGQPMBWYDCAilwacZtiD81fwxIDXb+1lmzu6f1DVA74fk/vYf1/rTgaaApb1ta9uU3GObgyVf9uq2gw8CVzkW3QRsNL38zSgOCSvZcAkX/sngLOBt0VktYiUDKNPxkQku5DHmNFtK7BCVVeENvj+Yv4tcBpQpqqdIlIFBJ4WcOu2o3eBdBFJChgcTO1n/a3AjD7a9gNJAc8n9bJO6O/xAPBdEVkDjAdeDNjPalU9vbcdqeprwHkiMg74EvDXAfptTNSxIwbGjB7jRCQx4BGH88V/lYgUi+MIETlHRFKAI3C+MN8DEJHP4RwxcJ2qvg2sw7mgMd73l/fSfl5yD/A5ETlNRGJEZIqIHO9rqwIuEpFxIlLEocP+/XkK5+jA94G/qGqXb/nfgVwRucS3vXEicrKInODr5zIROVJVDwL7gM7D/+2NiWw2MDBm9HgKaA143KSq63CuM/glsBvYDFwOoKobgJ8DZUAjMAf4Vxj7uwwoAXYBPwT+gnP9Qw+q+irwOeA2YC+wGueLHeA7OEcTduNcQPnngXbsu57gYeCjgev7TjOcgXN6YTvOqZCfAt0Xa14C1IvIPuAq4LOD/WWNiRZW4MgYMyJE5C/Am6r6Xa/7YowZOjtiYIwZEt8h+hm+UwNLgPOAR73ulzFmeOziQ2PMUE3COZw/AWgA/ktVX/e2S8aY4bJTCcYYY4zxc+1Ugoj8XkR2isj6PtpFRO4Ukc0i8m9fhbfutiW+UqabReR6t/pojDHGmGBuXmPwB2BJP+1nATN9j+XAbwBEJBb4la99NnCxr+SpMcYYY1zm2jUGqrpGRLL7WeU84D51zmW8IiJH+Wq8ZwObVXULgIg86Ft3w0D7nDhxomZn97fL6PPBBx8QHx/vdTeijuXuDcvdG5Z7+HVnXlFR8b6qHj2S2/by4sMpBJdQbfAt6215cV8bEZHlOEccmDx5MrfccktQ+6RJk8jJyaG2tpa8vDzWrFnTYxslJSXU1dWRlZVFY2MjW7cGV3adMmUKWVlZ1NXVkZuby9q1a3tso7S0lNraWnJycmhoaGDbtm1B7VOnTiUzM5OGhgZycnIoKyvrsY0FCxZQU1NDbm4udXV17NixI6g9Ozub9PR0GhsbycrKory8PDQLFi5cSHV1NXl5edTW1tLY2Ejg/DjTp08nJSWFpqYmMjMzWbduXdA24uLiKC0tpbKykrlz51JTU8OuXbuC1pk5cyYJCQm0tLSQnp5OZWVwufmEhARKSkqoqKhg3rx5VFdXs3v37qB1Zs2aRWxsLG1tbaSkpFBdXR3UPn78eIqLi/3bqKysZN++fUHrzJ49m87OTrq6ukhISGD9+uCzVsnJyRQVFfm3sW7dOlpaWoLWyc/Pp729nZiYGGJjY9mwIXj8mZqaSmFhoX8b5eXltLa2Bq1TUFBAc3MziYmJdHZ2snHjRlTVn3taWhoFBQX+bZSVldHeHny7f2FhIU1NTSQnJ9Pe3s6mTZuC2idMmEBeXh5VVVUUFhaydu1aOjo6gtYpKiqisbGR9PR0mpub2bJlS1B7RkYGubm51NTUUFBQwOrVqwm9xqi4uJiGhgYyMzNpamqivr4+qH20f54Cc3fj87Rz586gdezz1BKUu1ufp0D2edoalPnixYvf7vGiYXL14kPfEYO/q2qPamwi8iTwY9/kK4jI88B1wHTgTFXtnvr1EuAUVf3yQPsrKirS0A9mtNu0aRMzZ870uhtRx3L3huXuDcs9/LozF5EKVS0ayW17ecSggeAa5Vk4lcri+1huhiAzs69J84ybLHdvWO7esNzDz83MvSxw9Dhwqe/uhA8Be1X1XeA1YKaI5IhIPE5p0/6mczX9aGpq8roLUcly94bl7g3LPfzczNy1IwYi8gCwCJgoIg3Ad4FxAKp6F05d+LNxar8fwKmjjqp2iMiXgKeBWJw53Gvc6mekq6+vxy7IDD/L3RuWu/sOHjxIQ0MDbW1t/mVtbW09rhUwIysxMZGsrCzGjRsHuPted/OuhIsHaFfgi320PYUzcDDGGDOKNDQ0kJKSQnZ2tv9Cz+bmZlJSUjzuWeRSVXbt2uW/2NZtNleCMcaYQWtra2PChAlBdzsZd4kIEyZMCDpK4yYbGBhjjDksNigIv3BmbgOD3qxcCdnZEBPj/Ltypdc9GrJJkyZ53YWoZLl7w3L3Rlxc+G5w27NnD7/+9a+H9Nqzzz6bPXv2jHCPvOHme90GBqFWroTly+Htt0HV+Xf58jE7OAjH+SjTk+XuDcvdGwkJCWHbV38Dg87Ozn5f+9RTT3HUUUe50a1BGah/h8PN97oNDELdeCMcOBC87MABZ/kYVFtb63UXopLl7g3L3Rv9nvse4SOw119/PW+99RZz587l2muvZdWqVSxevJjPfOYzzJkzB4CPf/zjzJs3j7y8PO6++27/a7Ozs3n//fepr6/nhBNO4Atf+AJ5eXmcccYZvd5V8dBDD5Gfn09BQQELFiwAnC/3a665hjlz5nDiiSfyi1/8AoDnn3+ek046iTlz5vD5z3/eX40xOzub73//+5SWlvLQQw/x1ltvsWTJEubNm8epp57Km2++2ee++uPqe11VI+Yxb948HTYRVedYQfBDZPjb9kBnZ6fXXYhKlrs3LHf3bdiwoceyrq6u3le+/37VpKTg/5cmJTnLh6iurk7z8vL8z1988UVNSkrSLVu2+Jft2rVLVVUPHDigeXl5+v7776uq6rRp0/S9997Turo6jY2N1ddff11VVS+88EL905/+1GNf+fn52tDQoKqqu3fvVlXVX//613rBBRfowYMH/ftqbW3VrKws3bhxo6qqXnLJJXrbbbf59/nTn/7Uv82PfOQjWltbq6qqr7zyii5evLjPfYUKzL77vQ6s0xH+LvWy8uHodOyxzumD3paPQWvWrGHRokVedyPqWO7esNzD7OqroaqKzs5O4mJje7a/8gqEzGPAgQNwxRXw29/2vs25c+H22w+rG6ecckrQofU777yTRx55BICtW7eyadMmJkyYEPSanJwc5s6dC8C8efN6zGMAMH/+fC6//HI+9alPccEFFwDw3HPPcdVVV/mvq0hPT6e6upqcnBxyc3MBuOyyy/jVr37F1VdfDcCnP/1pAFpaWnj55Ze58MIL/fvoPrLQ27764+Z73QYGoVascK4pCD2dcPrp3vTHGGPGqtBBwUDLh+iII47w/7xq1Sqee+45ysrKSEpKYtGiRb2e6gi8LiI2NrbXUwl33XUX5eXlPPnkk8ydO5eqqqqgibq66QBzDnX3r6uri6OOOoqqqqpB7St0MBMuNjAItWyZ8++NN8I770BWFhx1FPzud875sW99C+xWHWOM8f9l39pXgaPs7N6PwE6bBqtWDWmXKSkpNDc399m+d+9e0tLSSEpK4s033+SVV14Z0n4A3nrrLYqLiykuLuaJJ55g69atnHHGGdx1110sWrSIuLg4mpqaOP7446mvr2fz5s0cd9xx/OlPf2LhwoU9tpeamkpOTg4PPfQQF154IarKv//9bwoKCnrdl1cDA7v4sDfLlkF9PXR1OYODigq45BL49rfhy1+GEbyy1BhjItaKFZCUFLwsKclZPkQTJkxg/vz55Ofnc+211/ZoX7JkCR0dHZx44ol85zvf4UMf+tCQ93XttdcyZ84c8vPzWbBgAQUFBVx55ZUce+yxnHjiiRQUFPDnP/+ZxMRE7r33Xi688ELmzJlDTEwMV111Va/bXLlyJffccw8FBQXk5eXx2GOP9bkvz4z0RQtePkbk4sO+dHaqXnedc/HMJz+p2trq3r5G0Isvvuh1F6KS5e4Ny919vV18uG/fvr5fcP/9qtOmORdwT5s2rAsPo11g9t3vdeziQw/FxMBPfwrHHANf+xq89x48+qhzmmEUKykp8boLUcly94bl7o3Ac/w9LFt26BStGTFuvtftVMLhuvpqeOABePllWLAAtm/3ukf9qqur87oLUcly94bl7o32Eb6Y0AzMzfe6DQyG4qKL4KmnoK4OSkrAV6BiNMrKyvK6C1HJcveG5e6N+Ph4r7sQddx8r9vAYKg++lFYs8a57aa01LlfdxRqbGz0ugtRyXL3huXujYMHD3rdhajj5nvdBgbDcdJJzimFtDT4yEfgySe97lEPW7du9boLUcly94bl7g0bGISfm+91GxgM1/Tp8K9/QV4enHce3Huv1z0yxhhjhswGBiMhIwNefNE5vfD5z8OPfuRUBTfGGDOihjPtMsDtt9/OgdDKtiaIDQxGSnIyPP44fPazTtVEK4RkjDEjbqwMDFSVrq4u1/fjBhsYjKT4ePjjH+Haa+FXv3LuXuhvOtIwmDJliqf7j1aWuzcsd2+MGzeuz7aVb6wk+/ZsYr4XQ/bt2ax8Y2SnXQa4+eabOfnkkznxxBP57ne/C8D+/fs555xzKCgoID8/n7/85S/ceeedbN++ncWLF7N48eJetz179mxOPPFErrnmGsC5yO/888+noKCAgoICXn75ZQBuvfVW8vPzyc/P53Zfaeju6Zz/+7//m8LCQrZu3cozzzxDSUkJhYWFXHjhhbS0tPS5r8Ph6nt9pCsmeflwtfLh4br1VqdK4qJFqnv2eNaNAwcOeLbvaGa5e8Nyd19vlQ/7mu76/n/fr0krkpSb8D+SViTp/f8euWmXn376af3CF76gXV1d2tnZqeecc46uXr1a/+///k+vvPJK/3p7fP8f7p56OdSuXbs0NzfXP4V099THn/rUp/xTKHd0dOiePXt03bp1mp+fry0tLdrc3KyzZ8/WyspKraurUxHRsrIyVVV977339NRTT9WWlhZVVf3JT36i3/ve9/rc10ACs+9+r2OVD8eQr30NJk2Cyy5zCiH94x8weXLYu1FXV8fs2bPDvt9oZ7l7w3IPr6v/eTVVO6ro6uoiJqbnAehXGl6hvTO4+NGBgwe44rEr+G1F79Muz500l9uXDH7a5WeeeYZnnnmGk046CXCmNt60aROnnnoq11xzDd/85jc599xzOfXUU/vdTmpqKomJiVx55ZWcc845nHvuuQC88MIL3HfffYAzC+ORRx7J2rVrOf/88/0VHy+44AJeeuklPvaxjzFt2jT//AyvvPIKGzZsYP78+QB88MEHlJSU9Lmvw+Hme91OJbjp4oudQkhbtsCHPwwbN4a9C93zg5vwsty9Ybl7o7dBAdBjUDDQ8qFQVW644Qaqqqqoqqpi8+bNXHHFFeTm5lJRUcGcOXO44YYb+P73v9/vduLi4nj11Vf5xCc+waOPPsqSJUv63WdfAstDqyqnn366v28bNmzgnnvuOax99cXV9/pIH4Lw8jGqTiUEqqhQzchQnTBB9ZVXwrprm1TGG5a7Nyx39x3OJErTbpsWdBqh+zHttmlD3v/777+vxx57rP/5008/raeccoo2NzerqmpDQ4M2Njbqtm3btNU32d0jjzyi5513nqqq5ufn65YtW3pst7m5WRsbG1XVOa2Qlpamqqqf/vSng04l7N27VysqKnTOnDm6f/9+bWlp0by8PP+phMDTHDt37tSpU6fqpk2bVFV1//79unHjxj73NRCbRCmSFBY6hZDOPBMWL4aHHoJzzvG6V8YY46oVp61g+RPLOXDw0F0ASeOSWHHayEy7fNZZZ3HzzTfzn//8xz+pUHJyMvfffz+bN2/m2muvJSYmhnHjxvGb3/wGgOXLl3PWWWdxzDHH8OKLL/q329zczHnnnUdbWxuqym233QbAHXfcwfLly7nnnnuIjY3lN7/5DSUlJVx++eWccsopAFx55ZWcdNJJ1NfXB/X16KOP5g9/+AMXX3yxfz6JH/7wh6SkpPS6r9FCtJ9DImNNUVGRrlu3zutu9G3nTjj7bKiqgt/+Fj73Odd3uWrVKhYtWuT6fkwwy90blrv7/vOf/3DCCScELWtubiYlJaXX9Ve+sZIbn7+Rd/a+w7FHHsuK01awbI7NtjgUgdl3v9dFpEJVi0ZyP3bEIJwyMmDVKvjEJ5xCSDt2wPXXg4jXPTPGGFcsm7PMBgJjjF18GG7JyfDEE8785N/6FnzlK1YIyRhjzKhhRwy8EB8P990HxxwDt9wCjY3wpz9BQsKI76q0tHTEt2kGZrl7w3L3RnJystddiDpuvtftiIFXYmLg5pvh5z93LkZcsgT27h3x3dTW1o74Ns3ALHdvWO7hEXptWpvHFV6jQWjmbr7XXR0YiMgSEdkoIptF5Ppe2tNE5BER+beIvCoi+QFt9SLyhohUicgovqJwmL7+dVi50pmhccEC2L59RDefk5Mzotszg2O5e8Nyd19iYiK7du0K+qJKcOFopzlEVdm1axeJiYn+ZW6+1107lSAiscCvgNOBBuA1EXlcVTcErPYtoEpVzxeR433rnxbQvlhV33erj6PGZz4DRx8NF1zgFEJ6+mmYNWtENt3Q0MDMmTNHZFtm8Cx3b1ju7svKyqKhoYH33nvPv+zgwYP9zpdghi8xMZGsrCz/czff625eY3AKsFlVtwCIyIPAeUDgwGA28GMAVX1TRLJFJFNVG13s1+h0+unOHQtnnw3z58OTT0Jx8bA3u23bNvsfpQcsd29Y7u4bN25cj79W7TbR8HPzve7mqYQpwNaA5w2+ZYGqgQsAROQUYBrQPSRS4BkRqRCR5S72c/SYN88phHTUUfCRjzjllI0xxpgwcvOIQW8354dWU/oJcIeIVAFvAK8DHb62+aq6XUQygGdF5E1VXdNjJ86gYTnA5MmTWbVqVVD7pEmTyMnJoba2lry8PNas6bEJSkpKqKurIysri8bGRrZu3RrUPmXKFLKysqirqyM3N5e1a9f22EZpaSm1tbXk5OTQ0NDAtm3bgtqnTp1KZmYmDQ0N5OTkUFZW1mMbCxYsoObAAXJfeAE96ywSli5l47XXssNXRzs7O5v09HQaGxvJysqivLw8NAsWLlxIdXU1eXl51NbW0tLSEpTJ9OnTSUlJoampiczMTEILQsXFxVFaWkplZSVz586lpqaGXbt2Ba0zc+ZMEhISaGlpIT09ncrKyqD2hIQESkpKqKioYN68eVRXV7N79+6gdWbNmkVsbCxtbW2kpKRQXV0d1D5+/HiKi4v926isrGTfvn1B68yePZvOzk66urpISEhg/fr1Qe3JyckUFRX5t7Fu3Tr/lKfd8vPzaW9vJyYmhtjYWDZs2BDUnpqaSmFhoX8b5eXltLa2Bq1TUFBAc3MziYmJdHZ2snHjxqDc09LSKCgo8G+jrKzMXwWtW2FhIU1NTSQnJ9Pe3s6mTZuC2idMmEBeXh5VVVUUFhaydu1aOjo6gtYpKiqisbGR9PR0mpub2bJlS1B7RkYGubm51NTUUFBQwOrVq3tc0FRcXExDQwOZmZk0NTX1qOQ22j9PgbkvWLCAmpoacnNzqaurY8eOHUHbGMrnaefOnUHr2OepJSh3tz5PgezztDUocze4VvlQREqAm1T1TN/zGwBU9cd9rC9AHXCiqu4LabsJaFHVW/rb56ivfHg4mpudQkjPPgs//jF885tDKoRkh/i8Ybl7w3L3huUefm5WPnTzVMJrwEwRyRGReOAi4PHAFUTkKF8bwJXAGlXdJyJHiEiKb50jgDOA4OFrpEtJgb//3bkw8YYb4Ktfha6uw97M1KlTXeicGYjl7g3L3RuWe/i5mblrpxJUtUNEvgQ8DcQCv1fVGhG5ytd+F3ACcJ+IdOJclHiF7+WZwCPOQQTigD+r6j/d6uuoFR/vFD6aNAluvdUphHTffYdVCCkzM9PFDpq+WO7esNy9YbmHn5uZu1r5UFWfAp4KWXZXwM9lQI/LKn13MhS42bcxIybGKYI0eTJccw289x488ggceeSgXt7Q0MDxxx/vcidNKMvdG5a7Nyz38HMzc6t8OFZ84xtw//3w0kuwcCG8++6gXmYFX7xhuXvDcveG5R5+bmZuA4OxZNkyp77B5s1OIaRBlMTs7e4H4z7L3RuWuzcs9/BzM3MbGIw1Z5zhFELav98phBRyi5UxxhgzHDYwGIuKipxCSKmpTiGkf/zD6x4ZY4yJEDYwGKuOO84ZHBx/PCxdCn/8o9c9MsYYEwFsYDCWZWY6pxUWL4bLL4ef/hRcKlhljDEmOrhW+dALEVX58HB88IEzMHjgAfjKV+C225zbHIGuri5iYmz8F26Wuzcsd29Y7uHXnflYq3xowiU+3rmV8WtfgzvvhIsvBl/t8JqaGo87F50sd29Y7t6w3MPPzcxdLXBkwigmxqmOOGWKUwjp/ffhkUfIzc31umdRyXL3huXuDcs9/NzM3I4YRJpvfMMpo7xmDSxYwDt2O6Mn6urqvO5CVLLcvWG5h5+bmdvAIBJ99rPOBEybN5P16U8PqhCSGVmhU/ya8LDcvWG5h5+bmdvAIFKdeSa8+CKxbW1OIaRXX/W6R8YYY8YAGxhEspNP5vVf/MKZwnnxYiuEZIwxZkA2MIhwrVlZTiGkWbPgYx9zpm02xhhj+mADgwiXnZ0NkyY5hZAWLoTLLoOf/cwKIbksOzvb6y5EJcvdG5Z7+LmZuQ0MIlx6errzQ2oqPPWUU+Pgm990ah50dXnbuQjmz92EleXuDcs9/NzM3AYGEa6xsfHQk+5CSFdfDXfcAZ/5jL8QkhlZQbmbsLHcvWG5h5+bmVuBowiXlZUVvKC7ENLkyXDddfDee/DII84RBTNieuRuwsJy94blHn5uZm5HDCJceW8FjkTg2mudCxHXrHGuPbD7kEdUr7kb11nu3rDcw8/NzG1gEM0uuQSeeAI2bYIPf9j51xhjTFSzgUG0W7IEXnwRmpudwcFrr3ndI2OMMR6ygYGBk092ah2kpMCiRfDPf3rdI2OMMR6xgUGEE5HBrThzpjM4yM2FpUudiZjMkA06dzOiLHdvWO7h52bmohFU6KaoqEjXrVvndTfGtn374Pzz4YUXnEJI11zjXKxojDFm1BGRClUtGslt2hGDCFddXX14L+guhHTRRc7tjF//uhVCGoLDzt2MCMvdG5Z7+LmZudUxiHB5eXmH/6KEBFi50imlfPvtzq2Mf/iDs9wMypByN8NmuXvDcg8/NzO3IwYRrra2dmgv7C6E9NOfwoMPwjnnOKcZzKAMOXczLJa7Nyz38HMzcxsYRLidO3cO/cUizumEP/4RVq927liwQkiDMqzczZBZ7t6w3MPPzcxtYGAGdumlTiGkjRutEJIxxkQ4GxiYwQkshDR/PtjdH8YYE5FsYGAG75RT4F//giOOcE4rPP201z0yxhgzwlwdGIjIEhHZKCKbReT6XtrTROQREfm3iLwqIvmDfa0ZnOnTp4/sBnNzoazMKYh07rnONM6mhxHP3QyK5e4Nyz383MzctYGBiMQCvwLOAmYDF4vI7JDVvgVUqeqJwKXAHYfxWjMIKSkpI7/RSZOcixEXLHAmYrrllpHfxxjnSu5mQJa7Nyz38HMzczePGJwCbFbVLar6AfAgcF7IOrOB5wFU9U0gW0QyB/laMwhNTU3ubLi7ENKnPuVM4WyFkIK4lrvpl+XuDcs9/NzM3M2BwRRga8DzBt+yQNXABQAicgowDcga5GvNIGRmZrq38YQEeOAB+MpX4Lbb4LOfhQ8+cG9/Y4iruZs+We7esNzDz83M3ax82FuB/dCJGX4C3CEiVcAbwOtAxyBf6+xEZDmwHGDy5MmsWrUqqH3SpEnk5ORQW1tLXl4ea9as6bGNkpIS6urqyMrKorGxka1btwa1T5kyhaysLOrq6sjNzWXt2rU9tlFaWkptbS05OTk0NDSwbdu2oPapU6eSmZlJQ0MDOTk5lJWV9djGggULqKmpITc3l7q6OnaE1AzIzs4mPT2dxsZGsrKyKC8vD82ChQsXUl1dTV5eHrW1tWzZsoXk5GT/OtOnTyclJYWmpiYyMzMJnVsiLi6O0tJSKisrmTt3LjU1NezatStonZkzZ5KQkEBLSwvp6elUfvzjTG1rY8bdd9NUW8umn/yE4o9+lIqKCubNm0d1dTW7d+8O2sasWbOIjY2lra2NlJSUHuU9x9/UXWkAACAASURBVI8fT3FxsX8blZWV7AspsDR79mw6Ozvp6uoiISGB9evXB7UnJydTVFTk38a6detoaWkJWic/P5/29nZiYmKIjY1lw4YNQe2pqakUFhb6t1FeXk5ra2vQOgUFBTQ3N5OYmEhnZycbN26kpaXFn3taWhoFBQX+bZSVldHe3h60jcLCQpqamkhOTqa9vZ1NIbeETpgwgby8PKqqqigsLGTt2rV0dHQErVNUVERjYyPp6ek0NzezZcuWoPaMjAxyc3OpqamhoKCA1atXEzpXSnFxMQ0NDWRmZtLU1ER9fX1Q+2j/PAXm7sbnKfTecVc+T5WVQe0JCQmUlJSM6s9Td+5ufZ4C2edpa1DmbnBtEiURKQFuUtUzfc9vAFDVH/exvgB1wIlA3uG8tptNotTTqlWrWLRoUXh29sc/whVXQEGBc5ohiv+KCGvuxs9y94blHn7dmY+1SZReA2aKSI6IxAMXAY8HriAiR/naAK4E1qjqvsG81oxCl13mFEJ6802nENLmzV73yBhjzGFybWCgqh3Al4Cngf8Af1XVGhG5SkSu8q12AlAjIm/i3IHw1f5e61ZfzQg66yynENK+fc7gwI7gGGPMmOLq7Iqq+hTwVMiyuwJ+LgNmDva15vDFxXkwgWZ3IaQzz3QKIT38MJxxRvj74SFPcjeWu0cs9/BzM3PXrjHwgl1jMMq8+65zBKGmxpm2edkyr3tkjDERZaxdY2BGgdArnMPqmGOcQkinnurcyvjzn3vXlzDzNPcoZrl7w3IPPzcztyMGEa6rq4uYGI/Hf+3tzgyNf/2rUwjp5pvB6z65bFTkHoUsd29Y7uHXnbkdMTCHraZmFFyz2V0I6ctfhltvdcooR3ghpFGRexSy3L1huYefm5nbFSMRLrSYimdiYuCOO2DyZLjhBnjvPfjb3yBCa6yPmtyjjOXuDcs9/NzM3I4YmPARgeuvh3vvhRdecO5YaGz0ulfGGGMC2MDAhN/ll8Pjj1shJGOMGYVsYGC8cfbZzlGDvXth/nyoqPC6R8YYY7CBQcSbObPX+lGjQ3GxUwhp/HjntMKzz3rdoxEzqnOPYJa7Nyz38HMzcxsYRLiEhASvu9C/WbPg5ZdhxgznKMLKlV73aESM+twjlOXuDcs9/NzM3AYGES50muFRafJkpxBSaalTCOnWW73u0bCNidwjkOXuDcs9/NzM3AYGES49Pd3rLgzOkUfCP/8JF14I3/gGXHMNdHV53ashGzO5RxjL3RuWe/i5mXmfAwMRSRSRo3tZniEiia71yIyoMVWqtLsQ0pe+5JRPvvTSMVsIaUzlHkEsd29Y7uHnZub9HTG4Ezi1l+WnA7e50x0T9WJj4c474Uc/cq43WLoUmpu97pUxxkSN/gYGpar6cOhCVV0JLHCvSybqiTjVEe+9F55/HhYvtkJIxhgTJv0NDGSIrzNmZFx+OTz2GGzYACeeCFOmOKWVs7Mj5u4FY4wZbfr7gt8pIqeELhSRk4H33OuSGUlj/jaic86Bb34Tdu6E7dtBFd5+G5YvH9WDgzGf+xhluXvDcg8/NzPvc9pl36Dgr8AfgO6ydEXApcBFqlruWq+GyKZdjlDZ2c5gIFRWFmzdGvbuGGPMaBHWaZdV9VWgGOeUwuW+hwDFo3FQYHpXEQmlht95p/flDQ1w2mnw6187RxNGkYjIfQyy3L1huYefm5n3ecRgLLIjBhGqryMGqalwzDGwcaNzwWJJCVxwgfPIyQl7N40xJtzCesRARN4QkX8HPKpF5HkR+bbVMRg7qqurve7C8K1YAUlJwcuSkpwjBW++CTU18P3vw4EDTmGk6dNh3jzndW++6UmXIyL3Mchy94blHn5uZt7fxYfnAksDHh8DrgEmAr9wrUdmRO3evdvrLgzfsmVw990wbZpzZGDaNOf5smVO++zZ8O1vw+uvO1M433wzxMc7y044wWn/znegqsq5eDEMIiL3Mchy94blHn5uZt7fNQZv9/J4XVWvxrkI0ZjwWbYM6uudMsn19YcGBaFmzHCOGpSVOdcg/OIXkJnpFEw66SQ47ji49lqnfQyXXDbGGLcMtR6B1TEwo9+UKU555RdfhB074Le/hdxcuOMO+PCH4dhj4ctfdto7OrzurTHGjApxfTWISGEvi9OAzwJrXOuRMW44+mi48krnsWcP/P3v8PDDcM898MtfwsSJcN558IlPOHc6xMd73WNjjPFEf3UMXgxZpMAuYBVwt6oedLdrh8/uSujp3Xff5ZhjjvG6G6PX/v3OrI5/+5szWGhudu52WLrUGSSceWbPCx8HwXL3huXuDcs9/Lozd+OuhD6PGKjq4r7aRCQTsOL1Y0BsbKzXXRjdjjjCGQB84hPQ1ubMzfC3vzmlmFeudAYFZ53ltJ9zjjNoGATL3RuWuzcs9/BzM/NBXysgIkeKyOdF5DnA5tgcI9ra2rzuwtiRmOh8+f/+986kTc89B5ddBv/6F3zmM87piHPPddp37ep3U5a7Nyx3b1ju4edm5v0ODERkvIh8WkQeA9YDtwI/BKa61iMzolJSUrzuwtgUF3eoquK2bbB2LXzxi7B+PVxxhXOnw0c/6rS/+26Pl1vu3rDcvWG5h5+bmfdX4GglUAucAfwSyAZ2q+oqVbX7vMYIKzwyAmJiYP58uPVWqKuDdeuciZ0aGpzBwpQph9rr6wHL3SuWuzcs9/DzqsBRPrAb+A/wpqp24lyAaEz0EjlUVfE//3GqLn7ve85FjN/4hlOKed48jr3/fs+qLhpjzHD0V+CoAPgUkAo8JyIvASkiMmmwGxeRJSKyUUQ2i8j1vbQfKSJP+Mot14jI5wLa6n1lmatExG41MKOPSHBVxc2b4Wc/g/h4pt9zj1N1MS8v7FUXjTFmOPq9xkBV31TV/1HVWcDXgD8Br4rIywNtWERigV8BZwGzgYtFZHbIal8ENvgGIYuAn4tI4A3ki1V17kjfimGMK2bM8FdVLPvrX+HOOyEjo2fVxVdesaqLxphRa9B3JajqOlX9OjANuGEQLzkF2KyqW1T1A+BB4LzQzeIchRAgGWgCrATdCBo/frzXXYhKMYFVFd99N7jqYknJoaqLq1ZZ1cURZO93b1ju4edm5oc17bKIVKpqbxURe1v3k8ASVb3S9/wSoFhVvxSwTgrwOHA8kAJ8WlWf9LXV4VzjoMD/qurdA+3TChyZUa+76uLf/uYUVmprc6oufvzjTq2Ej3zEqi4aYwYtrAWO+urDMNcNHYWcCVQBHwFmAM+KyEuqug+Yr6rbRSTDt/xNVe1RillElgPLASZPnsyqVauC2idNmkROTg61tbXk5eWxZk3Pas4lJSXU1dWRlZVFY2MjW7duDWqfMmUKWVlZ1NXVkZuby9q1a3tso7S0lNraWnJycmhoaGDbtm1B7VOnTiUzM5OGhgZycnIoKyvrsY0FCxZQU1NDbm4udXV17NixI6g9Ozub9PR0GhsbycrKory8PDQLFi5cSHV1NXl5edTW1lJfX09SQOW+6dOnk5KSQlNTE5mZmYQOpOLi4igtLaWyspK5c+dSU1PDrpB79mfOnElCQgItLS2kp6dTWRlc1iIhIYGSkhIqKiqYN28e1dXVPWYCmzVrFrGxsbS1tZGSktLjCtvx48dTXFzs30ZlZSX79u0LWmf27Nl0dnbS1dVFQkIC69evD2pPTk6mqKjIv41169bR0tIStE5+fj7t7e3ExMQQGxvLhg0bgtpTU1MpLCz0b6O8vJzW1tagdQoKCmhubiYxMZHOzk42btzIgQMH/LmnpaVRUFDg30bZjBm0f/WrxCxfzoRXX2XimjVkPPgg8rvf0ZWaSttHP8qWuXNpOvlkuhKdGc4nTJhAXl4eVVVVFBYWsnbtWjpCjjQUFRXR2NhIeno6zc3NbNmyJag9IyOD3NxcampqKCgoYPXq1YT+YVBcXExDQwOZmZk0NTVR77vLotto/zwF5u7G52nnzp1B69jnqSUod7c+T4F6fJ7Kymhvbw9ap7CwkKamJpKTk2lvb2fTpk1B7ZHweQp8r4+0wz1i8ENV/fYg1y0BblLVM33PbwBQ1R8HrPMk8BNVfcn3/AXgelV9NWRbNwEtqnpLf/u0IwZmzGprcwoqPfywU3WxqWnIVReNMdHDjSMG/dUxOE5E5gcuU9Vvi8ipIjJjENt+DZgpIjm+CwovwjltEOgd4DTf/jKBWcAWETnCd5oBETkCp5bCesxhC/3rw4THYeeemHioquKOHfDss71XXbz33gGrLkYze797w3IPPzcz7+/iw9uB5l6Wt/ra+qWqHcCXgKdxaiH8VVVrROQqEbnKt9oPgA+LyBvA88A3VfV9IBNYKyLVwKvAk6r6z8H+UuaQ0MOFJjyGlfu4cYeqKjY0BFdd/PznD1Vd/M1veq26GM3s/e4Nyz383My8v2sMslX136ELVXWdiGQPZuOq+hTwVMiyuwJ+3o5zNCD0dVuAgsHsw5iIFhvrVFWcPx9+/nOorHQuXPzb3+C//9sZMHz4w3DBBc4jO9vrHhtjxrj+jhgk9tNm96YYE27dVRd/9COnquL69XDTTdDSElR1kR/9CEIu2DLGmMHqb2Dwmoh8IXShiFwBVLjXJWPMgEScqor/8z9OVcVNm5yqi+PGwY03wvHHB7db1UVjzCD1eVeC72LAR4APODQQKALigfNVdUevL/SQ3ZXQ086dO8nIyPC6G1HH09y3boVHH3VON7z0klNlcfp05+6GCy6AU05xJoaKQPZ+94blHn7dmYf1rgRVbVTVDwPfA+p9j++pasloHBSY3nV2dnrdhajkae5Tpx6qqvjuu3D33TBzJtx226Gqi1/5itMeYe8Pe797w3IPPzczH+yfDQp0YbMrjjldVpPfE6Mm94wM+MIXnCqL770H990HJ5/slGhevBiOOeZQ+wcfeN3bYRs1uUcZyz383My8vzoGU0SkHLgJmA4cB9wkIq+KyBTXemRGVEJCgtddiEqjMvejjoJLLoFHHnEGCX/9K5x2Gjz4oFNIKSPDaX/0UQipRjdWjMrco4DlHn5uZt7fEYNfAr9R1YWq+nVV/ZqqLvQt/7VrPTIjKrSsqQmPUZ97cjJceCE88IAzSHjiCTj/fHjySeffiROd9gcfhDF0j/qozz1CWe7h52bm/Q0MZqvqH0IXqup9OJMeGWMiQXfVxXvvhcZGp+ripZc6Fy5efLFTdXHp0t6rLq5c6dROiIlx/l250ovfwBgzgvorcBTb20IRiemrzRgzxnVXXfzoR+GXv4SyMufuhocfdmaFjI11rk244ALnFshrr4UDB5zXvv02LF/u/LxsmXe/gzFmWPo7YvCEiPzWN1cB4J+34C5CqhkaYyJQbCyUljp3M9TXw2uvwXXXwTvvHKq62D0o6HbggFNHwRgzZvV3xOA64MfA2yLyNs4dCdOAPwLfCkPfzAhITk72ugtRKeJyF4GiIuexYgXU1MCcOb2v+/bbTs2EGTOCH1OnQtzhzvR+eCIu9zHCcg8/NzMfcNplERmPc0eCAJtV9UC/L/CQFTgyJoyys51BQKjx42HaNNiyJfgWyLg45zWhA4YZM5wCTC7NLW9MJHOjwNGAw3dVbQXeCOjE6cB1qnr6SHbEuKOiooJ58+Z53Y2oExW5r1jhXFMQeDohKckpqLRsmVNxcds2eOutno/yctizJ3h7xxzT+6BhxgyYMME5ajGAqMh9FLLcw8/NzPsrifwRnOsJJgOPAj8C7sM5crBCVR92pUfDYEcMjAmzlSudawreecepqLhixeAvPGxqgs2bex84bN8evG5qas/BwnHHOf9OmeJcD2FMFHLjiEF/A4PXga8BZcBZOIOC76jqHSPZgZFkA4Oe1q1bR1HRiL5nzCBY7sN04ADU1fU+aKivh4MHD60bH+/MLDljBo0pKWSWlBwaPOTkOLdjGlfZ+z38ujMP96kEVdVVvp8fFZH3RvOgwPSupaXF6y5EJct9mJKSnNkh8/J6tnV2OhNF9TJomFBbC3/5y6F1RZwjCn2dokhLC9/vFMHs/R5+bmbe38DgKBG5IOC5BD4fjacSjDFRIDbWuYgxO9sp6Rxg7Ysvsig/v/cjDU89BTtC5n9LS+t70DB5csTOQmlMf/obGKwGlvbxXAEbGBhjRhcRp1Lj0UfDhz7Us72lxblbInTQ8Npr8H//FzzbZGKi/xRFj0d2Ntj8ACZC9TkwUNXPhbMjxhjjuuRkOPFE5xHq4EHnIsrejja88ELw3RciTl2G0Ashux+pqeH7nYwZYe5WGzGey8/P97oLUcly98awch837tAXeyhVZx6J3gYNjz3mTEQVaOLEvk9RTJo0qFsvxxJ7v4efm5nbwCDCtbe3e92FqGS5e8O13EWcL/RJk2D+/J7t+/b1fori5ZedGSq7ug6tm5TkFHTqbdAwbZozQBlj7P0efm5mbgODCBdjF095wnL3hme5p6bC3LnOI9QHHzgVIkNrNmzaBE8/DW1th9aNjXXqQfR1tGEwZXCHU1tiiOz9Hn5uZj6ogYGIfBjIDlzfN/2yGeVirfCLJyx3b4zK3OPjYeZM5xGqqwvefbf3UxQPPeQUgQqUkdF3oaejj4Y//zm4GmWYZrwclblHODczH3BgICJ/AmYAVUD3JbuKU/DIjHIbNmwgIyPD625EHcvdG2Mu95gYp87ClCmwYEHP9j17eh80rF7tHBkILFCXnAzt7cHFn8AZJFx3HZx7rnNkw4XrG8Zc7hHAzcwHc8SgCJitA822ZIwxZmQddRTMm+c8QrW1OVUgAwcMd97Z+3a2b3e2lZDgHHXIzOz5b+iyiROt1HSUGszAYD0wCXjX5b4YY4wZrMREOP5459Htscd6n/FywgS44QbnzoqdO51/330Xqqud56FHGcA5mjFx4sADiMxMYgJn0TRj3mAGBhOBDSLyKuC/DFJVP+Zar4wxxhy+vma8vOOOvq8xUHVOWTQ2Bg8cuv/t/rm83Pm3l1K8C8A5TTHAAML/s0unNMzIGMzA4Ca3O2Hck2qFVjxhuXsj6nPv/vI/nLsSRJzS0GlpwUcf+rJ/vzNACBhAbH/9dSbHxh5a9uabznUQu3b1vo3AUxoDndaYMMFOafTCzfd6n7MrjkU2u6IxxowiBw/C++/3fgSit2UdHT230X1Ko7cBROhAIiMj6mbTDPfsit07/RDwC+AEIB6IBfarapQPzceGiooK5vV24ZJxleXuDcvdG33mPm4cHHOM8xiIKuzePfAA4pVXnH/37+99O92nNAZzgeUYPqXh5nt9wCMGIrIOuAh4COcOhUuBmar6LVd6NAx2xMAYY6JE9ymN/o5AdP/b3ymNwQwgRuKUhkuFpzw5YgCgqptFJFZVO4F7ReTlwbxORJYAd+AcZfidqv4kpP1I4H7gWF9fblHVewfzWjM45eXlFBcXe92NqGO5e8Ny94YnuR9xhDP7ZU7OwOsePOjMZ9HfAGL7dnj99cM7pdHfoCJw9s2VK0e88JSbmQ9mYHBAROKBKhH5Gc5ti0cM9CIRiQV+BZwONACvicjjqrohYLUvAhtUdamIHA1sFJGVOIWUBnqtGYTW1lavuxCVLHdvWO7eGPW5jxsHkyc7j4F0dR26S6O/0xplZc7PfZ3SOPLIQ4OFigoIzejAAecIwhAHBm5mPpiBwSVADPAl4GvAVOATg3jdKcBmVd0CICIPAucBgV/uCqSIiADJQBPQARQP4rXGGGPMyIqJgfR053HCCQOvH3hKo6/TGn19ib/zzsj2fYQMODBQ1bdFZDxwjKp+7zC2PQXYGvC8AecLP9AvgceB7UAK8GlV7RKRwbzWGGOM8dZgTmlkZ/deeOrYY13r1nAM5q6EpcAtOHck5IjIXOD7gyhw1NulnqFXOp6JMwfDR3DmY3hWRF4a5Gu7+7ccWA4wefJkVq1aFdQ+adIkcnJyqK2tJS8vjzVr1vTYRklJCXV1dWRlZdHY2MjWrVuD2qdMmUJWVhZ1dXXk5uaydu3aHtsoLS2ltraWnJwcGhoa2LZtW1D71KlTyczMpKGhgZycHMrKynpsY8GCBdTU1JCbm0tdXR07duwIas/OziY9PZ3GxkaysrIoLy8PzYKFCxdSXV1NXl4etbW1tLS0BGUyffp0UlJSaGpqIjMzk9CLNePi4igtLaWyspK5c+dSU1PDrpALd2bOnElCQgItLS2kp6dTWVkZ1J6QkEBJSYn/qtnq6mp2794dtM6sWbOIjY2lra2NlJQUqqurg9rHjx9PcXGxfxuVlZXs27cvaJ3Zs2fT2dlJV1cXCQkJrF+/Pqg9OTmZoqIi/zbWrVtHS0hxlvz8fNrb24mJiSE2NpYNG4IPSqWmplJYWOjfRnl5eY9DeAUFBTQ3N5OYmEhnZycbN24Myj0tLY2CggL/NsrKynpMmVpYWEhTUxPJycm0t7ezadOmoPYJEyaQl5dHVVUVhYWFrF27lo6Q86BFRUU0NjaSnp5Oc3MzW7ZsCWrPyMggNzeXmpoaCgoKWL16NaEXHxcXF9PQ0EBmZiZNTU3U19cHtY/2z1Ng7m58nnbu3Bm0jn2eWoJyd+vzFGgsfp6mfPvbJH7lK8QE/K6dCQns+OIXmdjePqTPU+j/20fSYO5KqMD54l6lqif5lv1bVU8c4HUlwE2qeqbv+Q0AqvrjgHWeBH6iqi/5nr8AXI9zwWG/r+2N3ZXQ0+7du0lLS/O6G1HHcveG5e4Ny30QRviuhO7M3bgrYTATOneo6t4hbPs1YKaI5PguXrwI57RBoHeA0wBEJBOYBWwZ5GvNIDQ3N3vdhahkuXvDcveG5T4Iy5Y5k151dTn/DvNWRTczH9QkSiLyGSBWRGYCXwEGvF1RVTtE5EvA0zhHAH6vqjUicpWv/S7gB8AfROQNnNMH31TV9wF6e+3h/3omMcqqgI0Wlrs3LHdvWO7h52bmgxkYfBm4EWcCpQdwvqx/MJiNq+pTwFMhy+4K+Hk7cMZgX2sOX2dnp9ddiEqWuzcsd29Y7uHnZuYDnkpQ1QOqeqOqnqyqRb6f21zrkRlRoRfumPCw3L1huXvDcg8/NzPv84iBiPR7Tt+mXTbGGGMiT3+nEkpwagk8AJTT+y2ExhhjjIkg/Q0MJuGUJL4Y+AzwJPCAXQRojDHGRK4+rzFQ1U5V/aeqXgZ8CNgMrBKRL4etd2bY7N5ib1ju3rDcvWG5h5+bmfdb4EhEEoBzcI4aZOPUEvi9qm7r80UesgJHxhhjoklYCxyJyB9x6hUUAt/z3ZXwg9E6KDC9q6io8LoLUcly94bl7g3LPfzczLzPIwYi0gV0zycZuJIAqqqprvVqiOyIgTHGmGgS1iMGqhqjqim+R2rAI2U0DgpM73qbrMm4z3L3huXuDcs9/NzMfDBzJZgxLHTWMRMelrs3LHdvWO7h52bmNjAwxhhjjJ8NDIwxxhjjZwMDY4wxxvjZwCDCFRYWet2FqGS5e8Ny94blHn5uZm4DgwjX1NTkdReikuXuDcvdG5Z7+LmZuQ0MIlxycrLXXYhKlrs3LHdvWO7h52bmNjCIcHYbkTcsd29Y7t6w3MPPblc0Q7Zp0yavuxCVLHdvWO7esNzDz83MbWBgjDHGGD8bGBhjjDHGzwYGxhhjjPGzgUGEmzBhgtddiEqWuzcsd29Y7uHnZuZ9Trs8Ftm0yz11dXURE2Pjv3Cz3L1huXvDcg+/7szDOu2yiQxVVVVedyEqWe7esNy9YbmHn5uZ2xEDY4wxZoyyIwbmsK1du9brLkQly90blrs3LPfwczNzGxhEuI6ODq+7EJUsd29Y7t6w3MPPzcxtYGCMMcYYPxsYGGOMMcbPBgbGGGOM8bOBQYQrKhrRi1XNIFnu3rDcvWG5h5+bmbs6MBCRJSKyUUQ2i8j1vbRfKyJVvsd6EekUkXRfW72IvOFrs3sQh6ixsdHrLkQly90blrs3LPfwczPzOLc2LCKxwK+A04EG4DUReVxVN3Svo6o3Azf71l8KfE1VmwI2s1hV33erj9EgPT3d6y5EJcvdG5a7Nyz38HMzczePGJwCbFbVLar6AfAgcF4/618MPOBif6JSc3Oz112ISpa7Nyx3b1ju4edm5m4ODKYAWwOeN/iW9SAiScAS4G8BixV4RkQqRGS5a72McFu2bPG6C1HJcveG5e4Nyz383MzctVMJgPSyrK/6y0uBf4WcRpivqttFJAN4VkTeVNU1PXbiDBqWA0yePJlVq1YFtU+aNImcnBxqa2vJy8tjzZoem6CkpIS6ujqysrJobGxk69atQe1TpkwhKyuLuro6cnNze604VVpaSm1tLTk5OTQ0NLBt27ag9qlTp5KZmUlDQwM5OTmUlZX12MaCBQuoqakhNzeXuro6duzYEdSenZ1Neno6jY2NZGVlUV5eHpoFCxcupLq6mry8PGpra2lpaQnKZPr06aSkpNDU1ERmZiahJaTj4uIoLS2lsrKSuXPnUlNTw65du4LWmTlzJgkJCbS0tJCenk5lZWVQe0JCAiUlJVRUVDBv3jyqq6vZvXt30DqzZs0iNjaWtrY2UlJSqK6uDmofP348xcXF/m1UVlayb9++oHVmz55NZ2cnXV1dJCQksH79+qD25ORkioqK/NtYt24dLS0tQevk5+fT3t5OTEwMsbGxbNiwIag9NTWVwsJC/zbKy8tpbW0NWqegoIDm5mYSExPp7Oxk48aNQbmnpaVRUFDg30ZZWRnt7e1B2ygsLKSpqYnk5GTa29vZtGlTUPuECRPIy8ujqqqKwsJC1q5d26PASVFREY2NjaSnp9Pc3NzjfxwZGRnk5uZSU1NDQUEBq1evJrQkenFxMQ0NDWRmZtLU1ER9fX1Q+2j/PAXm7sbnaefOnUHr2OepJSh3tz5PgezztDUocze4NleCiJQAN6nqmb7nNwCo6o97WfcR4CFV/XMf27oJaFHVW/rbp82V0NOqVatYtGiR192IOpa7Nyx3b1jum/FMiQAAFu1JREFU4ded+VibK+E1YKaI5IhIPHAR8HjoSiJyJLAQeCxg2REiktL9M3AGsD70tcYYY4wZWa6dSlDVDhH5EvA0EAv8XlVrROQqX/tdvlXPB55R1f0BL88EHhGR7j7+WVX/6VZfI1lGRobXXYhKlrs3LHdvWO7h52bmNu1yhOvo6CAuzs1LSUxvLHdvWO7esNzDrzvzsXYqwYwCNTU1XnchKlnu3rDcvWG5h5+bmdsRA2OMMWaMsiMG5rCtXr3a6y5EJcvdG5a7Nyz38HMzcxsYRLhIOiI0llju3rDcvWG5h5+bmdvAwBhjjDF+NjAwxhhjjJ8NDIwxxhiXrXxjJdm3ZxPzvRiyb89m5Rsrve5Sn+yuhAjX2trK+PHjve5G1LHcvWG5e8Ny79/KN1ay/InlHDh4wL8saVwSdy+9m2Vzlg1pm92Zu3FXglWkiHANDQ3MnDnT625EHcvdG5a7NyIh946uDto62vyP9o72Qz93tvfZFtrub+s89Pwfm/9BW0db0P4OHDzAjc/fOOSBgZuZ28AgwmVmZnrdhahkuXvDcnf+Or3x+Rt5Z+87HHvksaw4bcWQv3wGazi5q2rPL+XevmgH8SXdo+0w1u3SrmHnEB8bT2JcIolxiSTEJvh/Dh0UdHtn7ztD3peb73UbGES4pqYmUlNTve5G1LHcvRHtuYcesn5779ssf2I5QK+DA1XlYNfBYX8JN+5qJPGIxB5/KQ/2L25l+Ke0A7+IE+ISenxBp8SncHTS0YfaYvtet7ftDNQeHxtPjPR+2V727dm8vfftHsuPPfLYIf++br7XbWAQ4err68nOzva6G1HHcvfGWM/9YOdBWjtaOXDwAK0Hff8exvN7q+4NOo8NziHrzz36OX6w+ge9fkmPhPiYeJLik/r84kxNSCXjiIzgL+/D+BIe6As6PjYe36R7o9KK01b0eo3BitNWDHmbbr7XbWBgjDH96OjqGNKXdODzwa7b0dUxpD4mxiWSNC6J/Qf399p+sOsgBZMK/H8pD+dLOLR9XMw4Vq9ezaJFi4aRcmTrPloT7lM8Q2UDA2PMkHlxPhugs6uz1y/WN/a+wcG3Dh7el3jgl3cv6x7sOjikPibEJjB+3HiSxiUxPs75N2lcEuPHjSczOdO/zP/vuKE9T4xL9B/C7uuQ9bQjp/GXT/5lWJmb4Vk2Z9moHQiEsoGBMWZIejuf/YXHv0BzWzNn5549tMPh/f3FHbDsg84P+u5YVd9N8bHxfX7RTkya2PNLfIhf2olxicTGxI5w4gNz45C1iT42MIhwkyZN8roLUWms5t7W0cbu1t00tTaxu835t6m1yb8scPmL9S/2+IJu7Wjlv576L3hq8PscFzOuzy/atMQ0pqRMcZ7H9f2F3L3s/R3vM2v6rD6/tL34sg4nrw5Zj9X3+1jmZuZW4CjCtbe3k5CQ4HU3oo6XuXdpF3vb9vb55R60zPdzd1trR2uf2xWEtPFppCWmkT4+nde2v9bnur9b+rsBv8S7l8XFjNzfJ/Z+94blHn7dmVuBI3PYamtrmTNnjtfdiDojkXvrwdYB/2rvrX1P255+b//q/ks8fXw6aePTOC79ONITnZ/Tx6f727rbu39OTUgNuh2rv/PZVxReMazffajs/e4Nyz383MzcjhhEuK6uLmJibEqMcOvOvbOrk73tew/rr/buZf3dShYjMUFf7j2+0EPaupeljU8jMS5xRH5HN8q8Dpe9371huYdfd+Z2xMActjVr1thtRMOkqrR2tPb9V3v3srZDP7+7+10OcIC9bXv7/ev9iHFHBH15507IHfDLPX18OikJKX0WUwmX0XgLlr3fvWG5h5+bmdvAwIwJI3FbXEdXB3va9vR7cV1vf8k3tTb1exV8rMQGfXlnHJFBWmcax087vsch+dDD9PGx8cONxlNj6RYsY8zg2MDAjHp93Rb3zp53KJla0udf8qF/1e9t39vvfpLjk4O+vE+YeEK/f7V3L0uJT+lRdW3VqlX2F5QxZkyygYEZ9a579roeZV5bO1r51gvf6rFuXExc0Jf3pORJzD569oAX1x2VeNSY/+vdGGNGgg0MzKjT2dVJWUMZj298nCdqn2B78/Ze1xOE5y99PugLPzk+eVTXTDfGmNHO7kqIcGPl/uJ97ft45q1neHzj4zy16Sl2te5iXMw4FmYvpGJ7Bbvbdvd4zbQjp1F/dX34OzsIYyX3SGO5e8NyDz836xjY/SURrq6uzusu9OntPW/zy1d/yRl/OoOJP5vIhQ9dyJObnuSsmWfx10/+lfeve59nL3mWX5z9C5LGJQW9drSXeR3NuUcyy90blnv4uZm5nUqIcFlZWV53wa9Lu3ht22s8UfsEj298nDd2vgHArAmz+GrxV/nYrI9RMrWkRyW80Xhb3EBGU+7RxHL3huUefm5mbgODCNfY2EhycrJn+9//wX6e2/IcT9Q+wd9r/07j/kZiJZbSY0u55fRbWDprKbkTcgfczli7Lc7r3KOV5e4Nyz383MzcBgYRbuvWrcyYMSOs+9y2bxt/r/07T9Q+wfN1z9PW0UZqQipnHXcWS3OXctbMs0gfnx7WPoWbF7kby90rlnv4uZm5DQzMsKkqVTuq/HcRVLxb8f/bu//gqOs7j+PPNwkEMAEMSJAkQIBsMOAEYw4axaAiisim58z9YeXGzp2WXj2v3vXHjI4dh5lKrde7m95Nb86h2l9XW23v2rnQgtWp/Cg1B0IkQrQs6PJjgyRiBBKESJLP/bHf7O0uSYSwu1+SvB4zO9l8vp/vN5998d3kzXe/3+8HgJJJJXzxxi8SDAS5ZeYtuhxQRGQIUGEgg3Ku6xybw5up21/Hbw78hsjpCIZRXVzN08ueJhgIUn5NuS4dFBEZYtJaGJjZCuBfgSzgOefct5OWfx3o/eA4G7gOuMY51/Zp60rmtZ5p5beh31IXquPVd1/lzPkzXDX6Ku6ccyffvO2brCxdydSrpvo9TBERuQxpKwzMLAv4d2A5EAHeMLM659zbvX2cc98BvuP1DwL/4BUFn7quXJzCwsJBr+uco+mDJjbs30BdqI4dkR04HEUTinig4gFqy2q5ddatKZutbzi5nNxl8JS7P5R75qUz83QeMVgEHHTOvQdgZi8CnwX6++P+OeDng1xX+nGpl7R80v0J2w5vixUDh04eAqBqehVrb11LbVktFQUV+ojgU+jyLX8od38o98wbqpcrFgJH476PAIv76mhm44EVwCOXuq4MLBwOU15ePmCftrNtbDywkQ2hDbx88GVOd55mbPZY7ph9B48veZxVgVVMz5ueoREPDxeTu6SecveHcs+8dGaezsKgr/9S9nf/5SDwR+dc26Wua2ZrgDUA06dPZ8uWLQnLp02bRklJCaFQiPnz57Nt27YLtlFdXU04HKaoqIiWlhaOHj2asLywsJCioiLC4TCBQIDt27dfsI0lS5YQCoUoKSkhEonQ3NycsLy4uJiCggIikQglJSXU19dfsI2amhqampoIBAKEw2GOHz+esHzWrFnk5+fT0tJCUVERO3bsSM6CpUuX0tjYyPz58wmFQrS2ttLa2hrrM3v2bPLy8tgV3sXOUzv5ReMv2HdqHz30kD8mn5opNXyh5gtMPjWZ6qpqmpqaCO0OESIU20ZpaSk5OTl0dHSQn59PQ0NDwjhycnKorq5m9+7d3HjjjTQ2NvLRR4m3NC4rKyMrK4tz586Rl5dHY2NjwvJx48axePHi2DYaGho4ffp0Qp/y8nK6u7vp6ekhJyeHffv2JSzPzc2lqqoqto1du3bR0dGR0GfBggV0dnYyatQosrKyePvtxINSEyZMoLKyMraNHTt2cPbs2YQ+FRUVtLe3M3bsWLq7u9m/fz9ALPerr76aioqK2Dbq6+vp7OxM2EZlZSVtbW3k5ubS2dnJgQMHEpZPnjyZ+fPns2fPHiorK9m+fTtdXV0JfaqqqmhpaSE/P5/29nbee++9hOVTp04lEAjQ1NRERUUFW7duJfmW6IsXLyYSiVBQUEBbWxuHDh1KWD4U3k+9uafr/RSv9/3U1tZGQUEBybdkz87OZsmSJTQ0NLBw4UKampr48MMPE/oMl/dTa2trWt9PvfR++v/3U/L+mCppmyvBzKqBtc65u7zvHwdwzj3dR99fA790zv3sUteNp7kSLtQ7/W9XTxevH309dklh6MPoH/qKggqCgSDBsiBV06sYZbpLdipo2mV/KHd/KPfM6808HXMlpPOIwRtAqZmVAM3AfcD9yZ3MbCKwFPjLS11XBnbq3Ck2t27muV89x8YDG/no3EeMyRrDbbNu48uLvsyqwCpmTprp9zBFROQKkrbCwDnXZWaPAL8jesnhD5xzTWb2N97yZ72u9wKvOOfOfNq66RrrcBL+KBybi2Dr4a109XQxedxkastqCQaC3DnnTvJy8vwepoiIXKHSeh8D59xGYGNS27NJ3/8I+NHFrCsX6u7pZmfzzlgx0PRBtH66bsp1fLX6qxSeKeTh4MNkjcryeaQiIjIU6M6HQ1DHJx28+u6rsYmJPvj4A7Isi5qZNTxU+RDBQJA5+dF7aG/ZskVFgYiIXLS0nXzoh+F88mHkdIQN+zewIbSB18Kv0dndyaSxk7h77t3UltWyYu4KJo2ddMF6XV1dZGer/ss05e4P5e4P5Z55vZkPtZMP5TL0uB4a3m+IFQNvHn8TgDlXz+HhP3uY2rJabi6+mdFZowfcTigU0vXFPlDu/lDu/lDumZfOzFUYXEHOnj/La+HXYhMTHWs/xigbxU3FN/HMHc8QDASZN2XeJd11sKSkJI0jlv4od38od38o98xLZ+YqDHx2vON4wsREZ7vOkjsmlxVzVxAMBFlZupIp46cMevuRSITS0tIUjlguhnL3h3L3h3LPvHRmrsIgw5xz7G3dG5uLYGfzTgBmTJzBgzc8SLAsyNKZS8nJzknJz2tubtYb1gfK3R/K3R/KPfPSmbkKgwzo7Opk6+GtsWLgyKkjACwqXMRTtz1FsCzI9VOv18REIiLiOxUGaXLi4xMJExN1fNLBuOxxLJ+znCdrnuSewD1My53m9zBFREQSqDBIEeccfzrxJzaEolcRvH70dXpcD9PzpnP/gvupLavl9pLbGTd6nN9DFRER6ZcKg8twvvs8249sjxUDB9sOAnDDtBv4xi3foLaslsprK339iKC4uNi3nz2SKXd/KHd/KPfMS2fmKgz68MLeF3ji909w5NQRZkycwbpl61h9/WoATp47ycsHX6Zufx2bDm7i5LmT5GTlcHvJ7XzlM19hVWAVxROvnDdJQUGB30MYkZS7P5S7P5R75qUzcxUGSV7Y+wJrNqzh4/MfA3D41GEeqnuITaFNHOs4xh+O/IGuni6uGX8N9867l2AgyPI5y8kdk+vzyPsWiUSYN2+e38MYcZS7P5S7P5R75qUzc90SOcms787i8KnDfS5bMHUBwUCQYCDIosJFQ2IOgs7OTnJyUnPpo1w85e4P5e4P5Z55vZmn45bIo1K5seGg91LCZIax90t7+dayb1FdXD0kigKA+vp6v4cwIil3fyh3fyj3zEtn5ioMksyYOOOS2kVERIYTFQZJ1i1bx/jR4xPaxo8ez7pl63wakYiISOaoMEiy+vrVrA+uZ+bEmRjGzIkzWR9cH7sqQUREZDjTVQl9WH39ahUCIiIyIumqhGGup6eHUaN0YCjTlLs/lLs/lHvm9WauqxLkkjU1Nfk9hBFJuftDuftDuWdeOjNXYTDMBQIBv4cwIil3fyh3fyj3zEtn5ioMhrlwOOz3EEYk5e4P5e4P5Z556cxchcEwd/z4cb+HMCIpd38od38o98xLZ+YqDERERCRGhYGIiIjEqDAQERGRmGF1HwMz+wDoe2rEkWsKcMLvQYxAyt0fyt0fyj3zejOf6Zy7JpUbHlaFgVzIzHal+uYX8umUuz+Uuz+Ue+alM3N9lCAiIiIxKgxEREQkRoXB8Lfe7wGMUMrdH8rdH8o989KWuc4xEBERkRgdMRAREZEYFQZDkJkdMrO9ZrbHzHZ5bflm9qqZHfC+Xh3X/3EzO2hm+83srrj2G73tHDSzfzMz8+P1XKnM7Adm1mpm++LaUpazmeWY2Ute+w4zm5XJ13el6if3tWbW7O3ze8xsZdwy5X6ZzKzYzDab2Ttm1mRmj3rt2t/TaIDc/d3fnXN6DLEHcAiYktT2j8Bj3vPHgGe85+VAI5ADlADvAlnesp1ANWDAJuBuv1/blfQAaoBKYF86cgYeBp71nt8HvOT3a74SHv3kvhb4Wh99lXtqMr8WqPSe5wEhL1vt7/7k7uv+riMGw8dngR97z38M/Hlc+4vOuU7nXBg4CCwys2uBCc65ehfdY34St44AzrltQFtScypzjt/WfwHLdNSm39z7o9xTwDn3vnOuwXveDrwDFKL9Pa0GyL0/GcldhcHQ5IBXzGy3ma3x2gqcc+9DdGcDpnrthcDRuHUjXluh9zy5XQaWypxj6zjnuoBTwOS0jXzoe8TM3vI+aug9pK3cU8w71HwDsAPt7xmTlDv4uL+rMBiabnbOVQJ3A39rZjUD9O2rMnQDtMvgDCZn/RtcvP8A5gALgfeBf/balXsKmVku8N/A3zvnTg/UtY825T5IfeTu6/6uwmAIcs4d8762Ar8GFgEt3uEkvK+tXvcIUBy3ehFwzGsv6qNdBpbKnGPrmFk2MJGLP4Q+ojjnWpxz3c65HuD7RPd5UO4pY2ajif5xesE59yuvWft7mvWVu9/7uwqDIcbMrjKzvN7nwJ3APqAO+LzX7fPA/3jP64D7vDNTS4BSYKd3WLDdzD7jfd70QNw60r9U5hy/rb8AXvM+H5QkvX+cPPcS3edBuaeEl9HzwDvOuX+JW6T9PY36y933/d3vszL1uOSzWGcTPSu1EWgCnvDaJwO/Bw54X/Pj1nmC6Nmr+4m78gCo8na4d4Hv4d3wSo9YPj8nehjvPNGq+8FU5gyMBX5J9ASincBsv1/zlfDoJ/f/BPYCb3m/6K5V7inNfAnRw8tvAXu8x0rt777l7uv+rjsfioiISIw+ShAREZEYFQYiIiISo8JAREREYlQYiIiISIwKAxEREYlRYSAyApjZ5LiZ2o4nzdw25iK38UMzK7vMcRSb2UuXsw0RSS9drigywpjZWqDDOfdPSe1G9HdCjy8DE5Ergo4YiIxgZjbXzPaZ2bNAA3Ctma03s13e/PBPxvXdbmYLzSzbzE6a2bfNrNHM6s1sah/bvt1bvsfMGry7ds41sz3e8h/GHbU4YWZPeO2PmdlObwKZJ5O3KyLppcJARMqB551zNzjnmoHHnHNVQAWw3MzK+1hnIrDVOVcB1AN/3UefrwNrnHMLgRrgXPxC59xfecvuBU4APzGzlcAMYDHRCWRuMrObUvIqReSiqDAQkXedc2/Eff85M2sgegThOqKFQ7KzzrlN3vPdwKw++vwR+K6Z/R3RueK7kzuY2Tiit2v9knPuKNG5P+4G3vR+/lwgMKhXJSKDku33AETEd2d6n5hZKfAosMg5d9LMfkr0XuvJPol73k0fv0ucc0+ZWR1wD/CGmd3KhdO9fh940Tm3uXcIwFPOuecH+2JE5PLoiIGIxJsAtAOnvRne7hrshsxsjnPuLefc00SPAJQlLX8UGJ10EuTvgAe9mUMxsyIzmzLYMYjIpdMRAxGJ1wC8TXSWtveIfhwwWF8zs1uAHqKzxL1C9PyB2HLg496TEYHvOeeeM7N5wP9GL5KgHbif6DkIIpIBulxRREREYvRRgoiIiMSoMBAREZEYFQYiIiISo8JAREREYlQYiIiISIwKAxEREYlRYSAiIiIxKgxEREQk5v8AlkTm4C+Jlo0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(sample_sizes, train_scores, 'o-', color=\"r\",\n",
    "         label=\"train scores\")\n",
    "plt.plot(sample_sizes, test_scores, 'o-', color=\"g\",\n",
    "         label=\"test scores\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Mean ROC-AUC\")\n",
    "plt.title('Learning curves')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(linestyle='-.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*При росте числа объектов в обучающей выборке точность модели на тестовой выборке увеличивается. В связи с этим стоит оставить все имеющиеся наблюдения для построения модели*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Часто несбалансированные по классам выборки приводят к различным проблемам при обучении моделей. Давайте попробуем по-разному обработать выборку, поиграть с распределением объектов по классам и сделать выводы о том, как соотношение классов влияет на качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Задайте веса объектам так, чтобы соотношение классов с учетом весов объектов изменилось. Попробуйте не менее трёх различных вариантов весов. Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:10\n",
      "\t fit_time : 7.413555224736531\n",
      "\t score_time : 0.6001218954722086\n",
      "\t test_precision : 0.0\n",
      "\t test_roc_auc : 0.6753525389999654\n",
      "1:15\n",
      "\t fit_time : 7.573759317398071\n",
      "\t score_time : 0.6102995872497559\n",
      "\t test_precision : 0.0\n",
      "\t test_roc_auc : 0.6792550477063045\n",
      "1:20\n",
      "\t fit_time : 7.753904581069946\n",
      "\t score_time : 0.6166502634684244\n",
      "\t test_precision : 0.0\n",
      "\t test_roc_auc : 0.6807561092146809\n",
      "1:30\n",
      "\t fit_time : 8.617196321487427\n",
      "\t score_time : 0.6467326482137045\n",
      "\t test_precision : 0.0\n",
      "\t test_roc_auc : 0.673215188189919\n",
      "1:40\n",
      "\t fit_time : 9.455897331237793\n",
      "\t score_time : 0.6967449188232422\n",
      "\t test_precision : 0.0\n",
      "\t test_roc_auc : 0.6721655017166387\n"
     ]
    }
   ],
   "source": [
    "scorings = ['precision', 'roc_auc']\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "weights = [10,15,20,30,40]\n",
    "\n",
    "for w in weights:\n",
    "    # применяется модель RandomForestClassifier, так как есть реализация балансировки классов\n",
    "    rf_model = RandomForestClassifier(n_estimators=100,\n",
    "                                      class_weight= {-1: 1, 1:w})\n",
    "    scores = cross_validate(rf_model, X, y, cv=cv, scoring=scorings)\n",
    "    print('1:{}'.format(w))\n",
    "    for key in scores.keys():\n",
    "        print('\\t',key, ':',scores[key].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Стратегия балансировки весов не дает сильного прироста и при разных запусках модели результаты получаются разными. Нет явной динамики увеличения качества при увеличении веса класса \"отток\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Примените к выборке технологию undersampling: для этого нужно убрать из обучения некоторое количество объектов большего класса таким образом, чтобы соотношение классов изменилось. Попробуйте не менее трёх различных вариантов undersampling (варианты могут отличаться как по количество отфильтрованных объектов, так и по принципу выборка объектов для отсеивания из выборки). Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 2190, 1: 2190})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_oto = RandomUnderSampler(sampling_strategy=1,random_state=0)\n",
    "X_oto, y_oto = rus_oto.fit_resample(X, y)\n",
    "\n",
    "Counter(y_oto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 10950, 1: 2190})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_otf = RandomUnderSampler(sampling_strategy=0.2,random_state=0)\n",
    "X_otf, y_otf = rus_otf.fit_resample(X, y)\n",
    "\n",
    "Counter(y_otf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 21900, 1: 2190})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus_ott = RandomUnderSampler(sampling_strategy=0.1,random_state=0)\n",
    "X_ott, y_ott = rus_ott.fit_resample(X, y)\n",
    "\n",
    "Counter(y_ott)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_to_one : mean gb roc auc  0.7152467629949333  mean rf roc auc  0.7003680073392966\n",
      "one_to_five : mean gb roc auc  0.7221736410833801  mean rf roc auc  0.6875682325222576\n",
      "one_to_ten : mean gb roc auc  0.7267834282020808  mean rf roc auc  0.6856412501824398\n"
     ]
    }
   ],
   "source": [
    "datasets = [('one_to_one',X_oto,y_oto),\n",
    "            ('one_to_five',X_otf,y_otf),\n",
    "            ('one_to_ten',X_ott,y_ott)]\n",
    "gb_model = GradientBoostingClassifier(n_estimators=200)\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "for data in datasets:\n",
    "    scores_gb = cross_validate(gb_model, data[1], data[2], cv=cv, scoring='roc_auc')\n",
    "    scores_rf = cross_validate(rf_model, data[1], data[2], cv=cv, scoring='roc_auc')\n",
    "    print(data[0],': mean gb roc auc ',scores_gb['test_score'].mean(),' mean rf roc auc ',scores_rf['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Видно, что при при отношении классов 1:1 при undersampling RandomForestClassifier работает лучше, однако для GradientBoostingClassifier это ен так важно ипри росте класса \"не отток\" он работает лучше*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Теперь перейдем к работе с признаками. Ранее вы реализовали несколько стратегий для обработки пропущенных значений. Сравните эти стратегии между собой с помощью оценки качества моделей кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка пропущенных значений сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/train_dataset.csv')\n",
    "data = data.drop(deleted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выберем только числовые признаки\n",
    "numeric = data.loc[:, 'Var1':'Var190'] \n",
    "y = data['label']\n",
    "# numeric.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7174940521050376"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Заполнение значениями близкими к 0 (так как в числовых признаках присутствуют 0)\n",
    "numeric_zeros = numeric.fillna(0.0001)\n",
    "scores_numeric_z = cross_validate(gb_model, numeric_zeros, y, cv=cv, scoring='roc_auc')\n",
    "scores_numeric_z['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7165795540187082"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Заполнение средними занчениями (если число разных признаков больше 1)\n",
    "numeric_means = pd.DataFrame()\n",
    "for var in numeric.columns:\n",
    "    un_val = numeric[var].unique().shape[0]\n",
    "    if un_val > 1:\n",
    "        numeric_means[var] = numeric[var].fillna(numeric[var].mean())\n",
    "    else:\n",
    "        numeric_means[var] = numeric[var].fillna(0.0001)\n",
    "scores_numeric_mean = cross_validate(gb_model, numeric_means, y, cv=cv, scoring='roc_auc')\n",
    "scores_numeric_mean['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7167593947185648"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Заполнение медианными занчениями (если число разных признаков больше 5)\n",
    "numeric_medians = pd.DataFrame()\n",
    "for var in numeric.columns:\n",
    "    un_val = numeric[var].unique().shape[0]\n",
    "    if un_val >= 5:\n",
    "        numeric_medians[var] = numeric[var].fillna(numeric[var].mean())\n",
    "    else:\n",
    "        numeric_medians[var] = numeric[var].fillna(0.0001)\n",
    "scores_numeric_median = cross_validate(gb_model, numeric_medians, y, cv=cv, scoring='roc_auc')\n",
    "scores_numeric_median['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Заполнение средними дает бучть более высокое качество*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = data.loc[:,'Var191':'Var229'].fillna('missing_value')\n",
    "cat_train, cat_test, y_train, y_test = train_test_split(cat_features,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OE_enc = OrdinalEncoder()\n",
    "OE_enc.fit(cat_train)\n",
    "cat_oe = pd.DataFrame(OE_enc.transform(cat_train), columns=cat_features.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6412541539685911"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cat_oe = cross_validate(gb_model, cat_oe, y_train, cv=cv, scoring='roc_auc')\n",
    "scores_cat_oe['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_features='auto')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(cat_oe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5010784899128485"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,gb_model.predict(OE_enc.fit_transform(cat_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 OHD + OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_vars = []\n",
    "oe_vars = []\n",
    "for var in cat_features.columns:\n",
    "    un_val = cat_features[var].unique().shape[0]\n",
    "    if un_val <= 100:\n",
    "        ohe_vars.append(var)\n",
    "    else:\n",
    "        oe_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OE_enc = OrdinalEncoder()\n",
    "OE_enc.fit(cat_train[oe_vars])\n",
    "# все таки использовать OHE!!!\n",
    "OHE_enc = OneHotEncoder()\n",
    "OHE_enc.fit(cat_train[ohe_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_transf = OE_enc.transform(cat_train[oe_vars])\n",
    "ohe_transf = OHE_enc.transform(cat_train[ohe_vars]).toarray()\n",
    "cat_oe_ohe = np.concatenate((oe_transf,ohe_transf ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426848149221719"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_oe_dum = cross_validate(gb_model, cat_oe_ohe, y_train, cv=cv, scoring='roc_auc')\n",
    "scores_oe_dum['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качетсво незначительно отличается от обычного OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Counter encoder (подсчитывается, как часто встречается данная категория)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_dict={}\n",
    "for var in cat_train.columns:\n",
    "    repl_dict[var] = dict(Counter(cat_train[var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_ce=pd.DataFrame()\n",
    "cat_test_ce=pd.DataFrame()\n",
    "for var in cat_train.columns:\n",
    "    cat_train_ce[var] = cat_train[var].map(repl_dict[var])\n",
    "    cat_test_ce[var] = cat_test[var].map(repl_dict[var])\n",
    "cat_test_ce = cat_test_ce.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6594024475592587"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cat_ce = cross_validate(gb_model, cat_train_ce, y_train, cv=cv, scoring='roc_auc')\n",
    "scores_cat_ce['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(cat_train_ce, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5039550221065272"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,gb_model.predict(cat_test_ce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качетсво на обучении выше, однако на тесте оно не увеличивается значительно и результат нельзя считать значительно лучшим чем обычное преобразование с помощью OrdinalEncoder. Поэтому в дальнейшем будет использоваться преобразование категориальных признаков с OrdinalEncoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7231237224353401"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# модель без отбора признаков\n",
    "scores_kbest = cross_validate(gb_model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "scores_kbest ['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5126750564593208"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(X_train , y_train)\n",
    "\n",
    "roc_auc_score(y_test,gb_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_kb = SelectKBest(k=100)\n",
    "X_SKB = sel_kb.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 100)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_SKB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SKB_test = sel_kb.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(cat_train_ce, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,gb_model.predict(OE_enc.fit_transform(cat_test_ce)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7245613854524601"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_kbest = cross_validate(gb_model, X_SKB, y_train, cv=cv, scoring='roc_auc')\n",
    "scores_kbest ['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.514487697811153"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(X_SKB , y_train)\n",
    "\n",
    "roc_auc_score(y_test,gb_model.predict(X_SKB_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 37)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SelectFromModel(estimator=GradientBoostingClassifier())\n",
    "model.fit(X_train, y_train)\n",
    "X_SFM = model.transform(X_train)\n",
    "X_SFM.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0047169811320754715"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_vars_dict = dict(zip(X_train.columns,model.estimator_.feature_importances_))\n",
    "important_vars = []\n",
    "for var in important_vars_dict.keys():\n",
    "    if important_vars_dict [var]>model.threshold_:\n",
    "        important_vars.append(var)\n",
    "        \n",
    "len(important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SFM_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7271286019247937"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_sfm = cross_validate(gb_model, X_SFM, y_train, cv=cv, scoring='roc_auc')\n",
    "scores_sfm['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5107341909831167"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(X_SFM , y_train)\n",
    "\n",
    "roc_auc_score(y_test,gb_model.predict(X_SFM_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Видно, что при отборе признаков качество не падает (оно немного увеличивается, но это может быть и нестабильность модели, то есть из раза в раз результаты могут отличаться ан сколько то сотых или тысячных). Поэтому можно использовать не все признаки а отобрать самые важные. При этом качетсво между двумя способами отличается не сильно, поэтому можно взять второй вариант - подбор с помощью модели, т.к. качество то же самое, но признаков меньше и работать с ними удобнее*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Самым значимыми оказались те параметры, в которых меньше всего пропусков. Раз качество модели значимо не уменьшается, будем использовать только эти признаки*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Var6      0.110167\n",
       "Var13     0.110000\n",
       "Var21     0.110167\n",
       "Var25     0.099600\n",
       "Var28     0.099633\n",
       "Var38     0.099600\n",
       "Var57     0.000000\n",
       "Var73     0.000000\n",
       "Var74     0.110000\n",
       "Var76     0.099600\n",
       "Var81     0.110167\n",
       "Var85     0.099600\n",
       "Var109    0.145633\n",
       "Var113    0.000000\n",
       "Var119    0.110167\n",
       "Var125    0.110000\n",
       "Var126    0.279000\n",
       "Var133    0.099600\n",
       "Var149    0.145633\n",
       "Var153    0.099600\n",
       "Var173    0.099600\n",
       "Var189    0.585100\n",
       "Var197    0.003100\n",
       "Var198    0.000000\n",
       "Var199    0.000100\n",
       "Var202    0.000000\n",
       "Var205    0.038300\n",
       "Var206    0.110167\n",
       "Var207    0.000000\n",
       "Var210    0.000000\n",
       "Var214    0.508233\n",
       "Var216    0.000000\n",
       "Var217    0.013833\n",
       "Var218    0.013833\n",
       "Var220    0.000000\n",
       "Var222    0.000000\n",
       "Var226    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_data = data[important_vars]\n",
    "imp_data.isna().sum()/imp_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var13</th>\n",
       "      <th>Var21</th>\n",
       "      <th>Var25</th>\n",
       "      <th>Var28</th>\n",
       "      <th>Var38</th>\n",
       "      <th>Var57</th>\n",
       "      <th>Var73</th>\n",
       "      <th>Var74</th>\n",
       "      <th>Var76</th>\n",
       "      <th>...</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>Var210</th>\n",
       "      <th>Var214</th>\n",
       "      <th>Var216</th>\n",
       "      <th>Var217</th>\n",
       "      <th>Var218</th>\n",
       "      <th>Var220</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var226</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>819.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>66.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626789</td>\n",
       "      <td>46</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>IYzP</td>\n",
       "      <td>7M47J5GA0pTYIFxg5uy</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7WwriaL</td>\n",
       "      <td>GTeH</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>68RJbH2</td>\n",
       "      <td>z9ub4Lm</td>\n",
       "      <td>5Acm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401.0</td>\n",
       "      <td>11064.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>220.08</td>\n",
       "      <td>1555200.0</td>\n",
       "      <td>6.475540</td>\n",
       "      <td>110</td>\n",
       "      <td>378.0</td>\n",
       "      <td>4148792.0</td>\n",
       "      <td>...</td>\n",
       "      <td>sYC_</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>_Xu4WQh</td>\n",
       "      <td>beK4AFX</td>\n",
       "      <td>ETNp</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>_oSvdH_</td>\n",
       "      <td>v5hz20V</td>\n",
       "      <td>FSa2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>343.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>230.56</td>\n",
       "      <td>575124.0</td>\n",
       "      <td>5.585132</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1566640.0</td>\n",
       "      <td>...</td>\n",
       "      <td>zm5i</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uZLuNea</td>\n",
       "      <td>uAgP</td>\n",
       "      <td>UYBR</td>\n",
       "      <td>kM3Ojel</td>\n",
       "      <td>20HE4Qn</td>\n",
       "      <td>Xa3G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>826.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>314.48</td>\n",
       "      <td>10165920.0</td>\n",
       "      <td>0.836573</td>\n",
       "      <td>92</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>itlM</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>7A3j</td>\n",
       "      <td>vbUyGBN</td>\n",
       "      <td>XTbPUYD</td>\n",
       "      <td>71BH</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>dRiNT4W</td>\n",
       "      <td>4XQyovK</td>\n",
       "      <td>453m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>me75fM6ugJ</td>\n",
       "      <td>uKAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mAja5EA</td>\n",
       "      <td>qwWo</td>\n",
       "      <td>cJvF</td>\n",
       "      <td>ImpYlW4</td>\n",
       "      <td>LTMqFbB</td>\n",
       "      <td>FSa2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Var6    Var13  Var21  Var25   Var28       Var38     Var57  Var73  Var74  \\\n",
       "0   819.0     92.0   60.0   56.0   66.88         0.0  0.626789     46   14.0   \n",
       "1  2401.0  11064.0  132.0    0.0  220.08   1555200.0  6.475540    110  378.0   \n",
       "2   343.0      0.0   72.0   16.0  230.56    575124.0  5.585132     20    0.0   \n",
       "3   826.0   2200.0  128.0   16.0  314.48  10165920.0  0.836573     92   91.0   \n",
       "4     NaN      NaN    NaN    NaN     NaN         NaN  0.051057     10    NaN   \n",
       "\n",
       "       Var76  ...  Var206               Var207  Var210   Var214   Var216  \\\n",
       "0     5344.0  ...    IYzP  7M47J5GA0pTYIFxg5uy    uKAI      NaN  7WwriaL   \n",
       "1  4148792.0  ...    sYC_           me75fM6ugJ    uKAI  _Xu4WQh  beK4AFX   \n",
       "2  1566640.0  ...    zm5i           me75fM6ugJ    uKAI      NaN  uZLuNea   \n",
       "3        0.0  ...    itlM           me75fM6ugJ    7A3j  vbUyGBN  XTbPUYD   \n",
       "4        NaN  ...     NaN           me75fM6ugJ    uKAI      NaN  mAja5EA   \n",
       "\n",
       "   Var217  Var218   Var220   Var222  Var226  \n",
       "0    GTeH    cJvF  68RJbH2  z9ub4Lm    5Acm  \n",
       "1    ETNp    UYBR  _oSvdH_  v5hz20V    FSa2  \n",
       "2    uAgP    UYBR  kM3Ojel  20HE4Qn    Xa3G  \n",
       "3    71BH    cJvF  dRiNT4W  4XQyovK    453m  \n",
       "4    qwWo    cJvF  ImpYlW4  LTMqFbB    FSa2  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = imp_data.loc[:,'Var198':].fillna('missing_value')\n",
    "num_means = dict(imp_data.loc[:,:'Var189'].mean())\n",
    "num_features = imp_data.loc[:,:'Var189'].fillna(num_means)\n",
    "\n",
    "OE_enc = OrdinalEncoder()\n",
    "OE_enc.fit(cat_features)\n",
    "cat_features = pd.DataFrame(OE_enc.transform(cat_features), columns=cat_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fin = pd.concat([cat_features,num_features],axis=1)\n",
    "y_fin = data.label\n",
    "X_tr, X_tst, y_tr, y_tst = train_test_split(X_fin,y_fin,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [0.05, 0.01, 0.2],\n",
    "    'n_estimators':[50,100,200],\n",
    "    'max_depth':[3,5,8]\n",
    "}\n",
    "grid_search = GridSearchCV(gb_model, params, scoring = 'roc_auc', n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(n_estimators=200), n_jobs=4,\n",
       "             param_grid={'learning_rate': [0.05, 0.01, 0.2],\n",
       "                         'loss': ['deviance', 'exponential'],\n",
       "                         'max_depth': [3, 5, 8],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_tr,  y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'loss': 'exponential',\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7300193529576398"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_fin = grid_search.best_estimator_.predict(X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999104424144725"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_tst,preds_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Качество на кросс-валидации улучшилось, но на тестовых данных практически не изменилось*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбор признаков был произведен в п.5 с помощью метода feature_importances_. Можно посмотреть ,какой вклад внес каждый из признаков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame.from_dict(important_vars_dict, orient='index', columns=['importances']).sort_values(by='importances', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importances</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>numb_of_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var126</th>\n",
       "      <td>0.278914</td>\n",
       "      <td>52</td>\n",
       "      <td>8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var218</th>\n",
       "      <td>0.077810</td>\n",
       "      <td>3</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var73</th>\n",
       "      <td>0.073311</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var113</th>\n",
       "      <td>0.065201</td>\n",
       "      <td>29243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var74</th>\n",
       "      <td>0.057347</td>\n",
       "      <td>320</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var205</th>\n",
       "      <td>0.030983</td>\n",
       "      <td>4</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var189</th>\n",
       "      <td>0.027849</td>\n",
       "      <td>96</td>\n",
       "      <td>17553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var81</th>\n",
       "      <td>0.027420</td>\n",
       "      <td>25997</td>\n",
       "      <td>3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var226</th>\n",
       "      <td>0.018369</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var13</th>\n",
       "      <td>0.018284</td>\n",
       "      <td>2211</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var210</th>\n",
       "      <td>0.016289</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var57</th>\n",
       "      <td>0.013665</td>\n",
       "      <td>19579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var207</th>\n",
       "      <td>0.012707</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var217</th>\n",
       "      <td>0.011835</td>\n",
       "      <td>10610</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var28</th>\n",
       "      <td>0.010029</td>\n",
       "      <td>3264</td>\n",
       "      <td>2989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var153</th>\n",
       "      <td>0.009476</td>\n",
       "      <td>22981</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var222</th>\n",
       "      <td>0.009323</td>\n",
       "      <td>3378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var6</th>\n",
       "      <td>0.009089</td>\n",
       "      <td>1236</td>\n",
       "      <td>3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var21</th>\n",
       "      <td>0.008007</td>\n",
       "      <td>595</td>\n",
       "      <td>3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var173</th>\n",
       "      <td>0.007732</td>\n",
       "      <td>5</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var214</th>\n",
       "      <td>0.007571</td>\n",
       "      <td>10834</td>\n",
       "      <td>15247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var198</th>\n",
       "      <td>0.007081</td>\n",
       "      <td>3378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var199</th>\n",
       "      <td>0.006927</td>\n",
       "      <td>3622</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var202</th>\n",
       "      <td>0.006891</td>\n",
       "      <td>5262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var133</th>\n",
       "      <td>0.006669</td>\n",
       "      <td>22911</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var149</th>\n",
       "      <td>0.006390</td>\n",
       "      <td>11820</td>\n",
       "      <td>4369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var85</th>\n",
       "      <td>0.006382</td>\n",
       "      <td>124</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var25</th>\n",
       "      <td>0.005633</td>\n",
       "      <td>224</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var76</th>\n",
       "      <td>0.005620</td>\n",
       "      <td>18345</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var197</th>\n",
       "      <td>0.005427</td>\n",
       "      <td>219</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var206</th>\n",
       "      <td>0.005291</td>\n",
       "      <td>22</td>\n",
       "      <td>3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var125</th>\n",
       "      <td>0.005269</td>\n",
       "      <td>8182</td>\n",
       "      <td>3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var109</th>\n",
       "      <td>0.005136</td>\n",
       "      <td>171</td>\n",
       "      <td>4369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var220</th>\n",
       "      <td>0.005022</td>\n",
       "      <td>3378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var119</th>\n",
       "      <td>0.004948</td>\n",
       "      <td>1210</td>\n",
       "      <td>3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var216</th>\n",
       "      <td>0.004890</td>\n",
       "      <td>1584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var38</th>\n",
       "      <td>0.004720</td>\n",
       "      <td>19073</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        importances n_unique numb_of_nan\n",
       "Var126     0.278914       52        8370\n",
       "Var218     0.077810        3         415\n",
       "Var73      0.073311      130           0\n",
       "Var113     0.065201    29243           0\n",
       "Var74      0.057347      320        3300\n",
       "Var205     0.030983        4        1149\n",
       "Var189     0.027849       96       17553\n",
       "Var81      0.027420    25997        3305\n",
       "Var226     0.018369       23           0\n",
       "Var13      0.018284     2211        3300\n",
       "Var210     0.016289        6           0\n",
       "Var57      0.013665    19579           0\n",
       "Var207     0.012707       14           0\n",
       "Var217     0.011835    10610         415\n",
       "Var28      0.010029     3264        2989\n",
       "Var153     0.009476    22981        2988\n",
       "Var222     0.009323     3378           0\n",
       "Var6       0.009089     1236        3305\n",
       "Var21      0.008007      595        3305\n",
       "Var173     0.007732        5        2988\n",
       "Var214     0.007571    10834       15247\n",
       "Var198     0.007081     3378           0\n",
       "Var199     0.006927     3622           3\n",
       "Var202     0.006891     5262           0\n",
       "Var133     0.006669    22911        2988\n",
       "Var149     0.006390    11820        4369\n",
       "Var85      0.006382      124        2988\n",
       "Var25      0.005633      224        2988\n",
       "Var76      0.005620    18345        2988\n",
       "Var197     0.005427      219          93\n",
       "Var206     0.005291       22        3305\n",
       "Var125     0.005269     8182        3300\n",
       "Var109     0.005136      171        4369\n",
       "Var220     0.005022     3378           0\n",
       "Var119     0.004948     1210        3305\n",
       "Var216     0.004890     1584           0\n",
       "Var38      0.004720    19073        2988"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances['n_unique'] = None\n",
    "importances['numb_of_nan'] = None\n",
    "for var in imp_data.columns:\n",
    "    importances.loc[var,'n_unique'] = imp_data[var].unique().shape[0]\n",
    "    importances.loc[var,'numb_of_nan'] = imp_data[var].isna().sum()\n",
    "importances.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Основываясь на данных таблицы, а также анализа, проведенного на прошлых неделях, видно что наибольший вклад вносят те переменные, которые сильнее всего коррелируют с целевым показателем. Также важным является относительно небольшое количество пропусков*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли межу этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.05, loss='exponential',\n",
       "                           n_estimators=200)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(learning_rate = 0.05,loss = 'exponential',max_depth = 3,n_estimators =200)\n",
    "model.fit(X_tr,  y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_prob = model.predict_proba(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pd.DataFrame(np.concatenate((preds_prob,np.vstack(y_tr.values)),axis=1),columns=['prob_not_churn','prob_churn','true_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = pd.concat([probs,X_tr.reset_index()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего объектов класса \"отток\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1773"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Всего объектов класса \"отток\":')\n",
    "objects[(objects.true_value == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Верно инициализированные объекты:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_not_churn</th>\n",
       "      <th>prob_churn</th>\n",
       "      <th>true_value</th>\n",
       "      <th>index</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>...</th>\n",
       "      <th>Var109</th>\n",
       "      <th>Var113</th>\n",
       "      <th>Var119</th>\n",
       "      <th>Var125</th>\n",
       "      <th>Var126</th>\n",
       "      <th>Var133</th>\n",
       "      <th>Var149</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19745</th>\n",
       "      <td>0.486676</td>\n",
       "      <td>0.513324</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1072</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>403368.00</td>\n",
       "      <td>285.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6154000.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10308480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13808</th>\n",
       "      <td>0.486282</td>\n",
       "      <td>0.513718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25584</td>\n",
       "      <td>195.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>334982.80</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>4803475.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10760400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23107</th>\n",
       "      <td>0.474785</td>\n",
       "      <td>0.525215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11659</td>\n",
       "      <td>2249.0</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>939.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21404.84</td>\n",
       "      <td>510.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>551110.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10194080.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>0.474004</td>\n",
       "      <td>0.525996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26501</td>\n",
       "      <td>3007.0</td>\n",
       "      <td>1714.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>153830.00</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>6048000.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9034920.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.470036</td>\n",
       "      <td>0.529964</td>\n",
       "      <td>1.0</td>\n",
       "      <td>464</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>2908.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.041707</td>\n",
       "      <td>-903368.00</td>\n",
       "      <td>460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.985090e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14147</th>\n",
       "      <td>0.462844</td>\n",
       "      <td>0.537156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26285</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>3483.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>-711184.00</td>\n",
       "      <td>650.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>3249640.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6327000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>0.453040</td>\n",
       "      <td>0.546960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11280</td>\n",
       "      <td>2641.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1040.000000</td>\n",
       "      <td>6256.72</td>\n",
       "      <td>6160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2898165.0</td>\n",
       "      <td>2.186240e+05</td>\n",
       "      <td>9766120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>186.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4811</th>\n",
       "      <td>0.451783</td>\n",
       "      <td>0.548217</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20386</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1462.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>-181497.20</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2428905.0</td>\n",
       "      <td>6.048000e+05</td>\n",
       "      <td>7550280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18842</th>\n",
       "      <td>0.448678</td>\n",
       "      <td>0.551322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6969</td>\n",
       "      <td>3326.0</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>169342.00</td>\n",
       "      <td>525.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>-0.596764</td>\n",
       "      <td>2140585.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10559640.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11207</th>\n",
       "      <td>0.437352</td>\n",
       "      <td>0.562648</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5020</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>2998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>51139.60</td>\n",
       "      <td>215.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>3025220.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9306720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15418</th>\n",
       "      <td>0.433085</td>\n",
       "      <td>0.566915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21112</td>\n",
       "      <td>3125.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>2991.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.041707</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3861.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.985090e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19347</th>\n",
       "      <td>0.429217</td>\n",
       "      <td>0.570783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24082</td>\n",
       "      <td>863.0</td>\n",
       "      <td>2382.0</td>\n",
       "      <td>5011.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1775468.00</td>\n",
       "      <td>795.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-0.596764</td>\n",
       "      <td>6104700.0</td>\n",
       "      <td>6.065780e+05</td>\n",
       "      <td>10383480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12312</th>\n",
       "      <td>0.421606</td>\n",
       "      <td>0.578394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10358</td>\n",
       "      <td>884.0</td>\n",
       "      <td>3339.0</td>\n",
       "      <td>2951.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>83492.40</td>\n",
       "      <td>410.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.596764</td>\n",
       "      <td>2191155.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9405320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>642.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>0.408629</td>\n",
       "      <td>0.591371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7785</td>\n",
       "      <td>2461.0</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>4612.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>930552.00</td>\n",
       "      <td>815.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5858450.0</td>\n",
       "      <td>1.036952e+06</td>\n",
       "      <td>10717840.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>0.403579</td>\n",
       "      <td>0.596421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2222</td>\n",
       "      <td>452.0</td>\n",
       "      <td>3505.0</td>\n",
       "      <td>4553.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-141154.80</td>\n",
       "      <td>450.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>4465.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6967400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>0.397395</td>\n",
       "      <td>0.602605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27801</td>\n",
       "      <td>3170.0</td>\n",
       "      <td>2037.0</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-432168.00</td>\n",
       "      <td>1630.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2193075.0</td>\n",
       "      <td>9.738400e+04</td>\n",
       "      <td>3086288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23017</th>\n",
       "      <td>0.388043</td>\n",
       "      <td>0.611957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26366</td>\n",
       "      <td>96.0</td>\n",
       "      <td>3168.0</td>\n",
       "      <td>2685.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>368726.00</td>\n",
       "      <td>615.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3624895.0</td>\n",
       "      <td>2.162993e+06</td>\n",
       "      <td>10125920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5037</th>\n",
       "      <td>0.387867</td>\n",
       "      <td>0.612133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27339</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3485.0</td>\n",
       "      <td>5080.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1760276.00</td>\n",
       "      <td>515.0</td>\n",
       "      <td>16254.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3741345.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10719120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17160</th>\n",
       "      <td>0.379577</td>\n",
       "      <td>0.620423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17368</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>3320468.00</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5043450.0</td>\n",
       "      <td>5.864600e+04</td>\n",
       "      <td>9358120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19416</th>\n",
       "      <td>0.368796</td>\n",
       "      <td>0.631204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5584</td>\n",
       "      <td>2849.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>3303.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-841732.00</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>5852700.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6972040.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prob_not_churn  prob_churn  true_value  index  Var198  Var199  Var202  \\\n",
       "19745        0.486676    0.513324         1.0   1072  1157.0  1655.0  1465.0   \n",
       "13808        0.486282    0.513718         1.0  25584   195.0  1480.0  4857.0   \n",
       "23107        0.474785    0.525215         1.0  11659  2249.0  2540.0   939.0   \n",
       "8241         0.474004    0.525996         1.0  26501  3007.0  1714.0   775.0   \n",
       "205          0.470036    0.529964         1.0    464  2035.0   172.0  2908.0   \n",
       "14147        0.462844    0.537156         1.0  26285  1081.0  3483.0   152.0   \n",
       "2873         0.453040    0.546960         1.0  11280  2641.0   291.0  1028.0   \n",
       "4811         0.451783    0.548217         1.0  20386  1400.0  1462.0   545.0   \n",
       "18842        0.448678    0.551322         1.0   6969  3326.0  2540.0   746.0   \n",
       "11207        0.437352    0.562648         1.0   5020  2044.0   396.0  2998.0   \n",
       "15418        0.433085    0.566915         1.0  21112  3125.0   694.0  2991.0   \n",
       "19347        0.429217    0.570783         1.0  24082   863.0  2382.0  5011.0   \n",
       "12312        0.421606    0.578394         1.0  10358   884.0  3339.0  2951.0   \n",
       "3592         0.408629    0.591371         1.0   7785  2461.0  1134.0  4612.0   \n",
       "2359         0.403579    0.596421         1.0   2222   452.0  3505.0  4553.0   \n",
       "5700         0.397395    0.602605         1.0  27801  3170.0  2037.0  1451.0   \n",
       "23017        0.388043    0.611957         1.0  26366    96.0  3168.0  2685.0   \n",
       "5037         0.387867    0.612133         1.0  27339    82.0  3485.0  5080.0   \n",
       "17160        0.379577    0.620423         1.0  17368  2421.0   536.0  1813.0   \n",
       "19416        0.368796    0.631204         1.0   5584  2849.0  1624.0  3303.0   \n",
       "\n",
       "       Var205  Var206  Var207  ...       Var109      Var113  Var119   Var125  \\\n",
       "19745     1.0     6.0    10.0  ...    32.000000   403368.00   285.0      0.0   \n",
       "13808     3.0     6.0    10.0  ...    16.000000   334982.80   280.0      0.0   \n",
       "23107     0.0     2.0     5.0  ...    24.000000    21404.84   510.0      0.0   \n",
       "8241      1.0     6.0    10.0  ...     8.000000   153830.00   200.0      0.0   \n",
       "205       1.0    21.0    10.0  ...    61.041707  -903368.00   460.0      0.0   \n",
       "14147     3.0    21.0    10.0  ...    32.000000  -711184.00   650.0      0.0   \n",
       "2873      3.0    21.0    10.0  ...  1040.000000     6256.72  6160.0      0.0   \n",
       "4811      3.0     6.0    10.0  ...    32.000000  -181497.20   170.0      0.0   \n",
       "18842     3.0    19.0    10.0  ...    32.000000   169342.00   525.0    297.0   \n",
       "11207     1.0     1.0    10.0  ...     8.000000    51139.60   215.0      0.0   \n",
       "15418     3.0     6.0    10.0  ...    61.041707        0.00     0.0   3861.0   \n",
       "19347     1.0     6.0    10.0  ...    48.000000  1775468.00   795.0    324.0   \n",
       "12312     3.0    21.0    10.0  ...    16.000000    83492.40   410.0      0.0   \n",
       "3592      3.0    21.0    10.0  ...    48.000000   930552.00   815.0      0.0   \n",
       "2359      3.0    21.0    10.0  ...    40.000000  -141154.80   450.0      0.0   \n",
       "5700      3.0     6.0    10.0  ...    40.000000  -432168.00  1630.0      0.0   \n",
       "23017     3.0    21.0    10.0  ...    32.000000   368726.00   615.0      0.0   \n",
       "5037      3.0     6.0    10.0  ...    32.000000  1760276.00   515.0  16254.0   \n",
       "17160     1.0     6.0    10.0  ...    24.000000  3320468.00   255.0      0.0   \n",
       "19416     1.0    21.0    10.0  ...     0.000000  -841732.00   210.0      0.0   \n",
       "\n",
       "          Var126     Var133        Var149      Var153  Var173      Var189  \n",
       "19745   4.000000  6154000.0  0.000000e+00  10308480.0     0.0  270.791998  \n",
       "13808  -8.000000  4803475.0  0.000000e+00  10760400.0     0.0  270.791998  \n",
       "23107   4.000000   551110.0  0.000000e+00  10194080.0     0.0  156.000000  \n",
       "8241   -8.000000  6048000.0  0.000000e+00   9034920.0     2.0  270.791998  \n",
       "205     4.000000        0.0  2.985090e+05         0.0     0.0  270.791998  \n",
       "14147  -8.000000  3249640.0  0.000000e+00   6327000.0     0.0  270.791998  \n",
       "2873    4.000000  2898165.0  2.186240e+05   9766120.0     0.0  186.000000  \n",
       "4811    4.000000  2428905.0  6.048000e+05   7550280.0     0.0  270.791998  \n",
       "18842  -0.596764  2140585.0  0.000000e+00  10559640.0     0.0  270.791998  \n",
       "11207  -8.000000  3025220.0  0.000000e+00   9306720.0     0.0  168.000000  \n",
       "15418  26.000000        0.0  2.985090e+05         0.0     0.0  156.000000  \n",
       "19347  -0.596764  6104700.0  6.065780e+05  10383480.0     0.0  270.791998  \n",
       "12312  -0.596764  2191155.0  0.000000e+00   9405320.0     0.0  642.000000  \n",
       "3592    4.000000  5858450.0  1.036952e+06  10717840.0     0.0  168.000000  \n",
       "2359   -8.000000     4465.0  0.000000e+00   6967400.0     0.0  270.791998  \n",
       "5700    4.000000  2193075.0  9.738400e+04   3086288.0     0.0  270.791998  \n",
       "23017   4.000000  3624895.0  2.162993e+06  10125920.0     0.0  144.000000  \n",
       "5037   22.000000  3741345.0  0.000000e+00  10719120.0     0.0  270.791998  \n",
       "17160   4.000000  5043450.0  5.864600e+04   9358120.0     0.0  270.791998  \n",
       "19416  -8.000000  5852700.0  0.000000e+00   6972040.0     0.0  138.000000  \n",
       "\n",
       "[20 rows x 40 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Верно инициализированные объекты:')\n",
    "objects[(objects.true_value == 1)&(objects.prob_churn >= 0.5)].sort_values(by='prob_not_churn', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объекты, отнесенные не верно отнесенные к классу \"не отток\" с вероятностью больше 98%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_not_churn</th>\n",
       "      <th>prob_churn</th>\n",
       "      <th>true_value</th>\n",
       "      <th>index</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>...</th>\n",
       "      <th>Var109</th>\n",
       "      <th>Var113</th>\n",
       "      <th>Var119</th>\n",
       "      <th>Var125</th>\n",
       "      <th>Var126</th>\n",
       "      <th>Var133</th>\n",
       "      <th>Var149</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16909</th>\n",
       "      <td>0.985350</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16197</td>\n",
       "      <td>34.0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>4208.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>53292.40</td>\n",
       "      <td>555.000000</td>\n",
       "      <td>47781.000000</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.963650e+06</td>\n",
       "      <td>604800.000000</td>\n",
       "      <td>1.048888e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>0.985323</td>\n",
       "      <td>0.014677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21104</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>3340.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>144296.00</td>\n",
       "      <td>925.000000</td>\n",
       "      <td>157401.000000</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.030220e+06</td>\n",
       "      <td>77959.000000</td>\n",
       "      <td>1.055156e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14202</th>\n",
       "      <td>0.985077</td>\n",
       "      <td>0.014923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25391</td>\n",
       "      <td>2607.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>592.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>176128.00</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>26172.000000</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>5.148350e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.073044e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>0.984915</td>\n",
       "      <td>0.015085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>542</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>778.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>84054.00</td>\n",
       "      <td>645.000000</td>\n",
       "      <td>43110.000000</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>2.367195e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.009420e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17194</th>\n",
       "      <td>0.984629</td>\n",
       "      <td>0.015371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14826</td>\n",
       "      <td>640.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>3108.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>-90806.40</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>85437.000000</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>5.999000e+04</td>\n",
       "      <td>126784.000000</td>\n",
       "      <td>1.360200e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21679</th>\n",
       "      <td>0.983909</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17875</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1267.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>18085.16</td>\n",
       "      <td>1230.000000</td>\n",
       "      <td>15651.000000</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>9.028350e+05</td>\n",
       "      <td>358225.000000</td>\n",
       "      <td>1.810816e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23935</th>\n",
       "      <td>0.983649</td>\n",
       "      <td>0.016351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>903</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>4647.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>-1809956.00</td>\n",
       "      <td>915.000000</td>\n",
       "      <td>8883.000000</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>1.785250e+05</td>\n",
       "      <td>74368.000000</td>\n",
       "      <td>4.200360e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9147</th>\n",
       "      <td>0.983372</td>\n",
       "      <td>0.016628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25115</td>\n",
       "      <td>2495.0</td>\n",
       "      <td>2803.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.041707</td>\n",
       "      <td>-1859604.00</td>\n",
       "      <td>912.161453</td>\n",
       "      <td>28341.154719</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>2.274060e+06</td>\n",
       "      <td>298509.021966</td>\n",
       "      <td>6.158730e+06</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12978</th>\n",
       "      <td>0.983102</td>\n",
       "      <td>0.016898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7163</td>\n",
       "      <td>158.0</td>\n",
       "      <td>1156.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127444.40</td>\n",
       "      <td>515.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>8.164550e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.052960e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>432.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12865</th>\n",
       "      <td>0.982560</td>\n",
       "      <td>0.017440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21292</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>-1849496.00</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>15075.000000</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>1.419900e+05</td>\n",
       "      <td>10010.000000</td>\n",
       "      <td>2.112720e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21257</th>\n",
       "      <td>0.982116</td>\n",
       "      <td>0.017884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28501</td>\n",
       "      <td>2327.0</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>51163.20</td>\n",
       "      <td>575.000000</td>\n",
       "      <td>18621.000000</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>1.436975e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.593880e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17306</td>\n",
       "      <td>2327.0</td>\n",
       "      <td>2791.0</td>\n",
       "      <td>3886.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>287952.40</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>49032.000000</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>6.894200e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.979160e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>0.981380</td>\n",
       "      <td>0.018620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13492</td>\n",
       "      <td>2324.0</td>\n",
       "      <td>3506.0</td>\n",
       "      <td>4537.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>216084.00</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>28341.154719</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>8.190800e+06</td>\n",
       "      <td>48629.000000</td>\n",
       "      <td>1.041268e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7407</th>\n",
       "      <td>0.981240</td>\n",
       "      <td>0.018760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17323</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2602.0</td>\n",
       "      <td>4466.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>107190.80</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>24165.000000</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>2.173735e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.021912e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>0.981105</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18550</td>\n",
       "      <td>82.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>-198978.40</td>\n",
       "      <td>1275.000000</td>\n",
       "      <td>87246.000000</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>2.867490e+06</td>\n",
       "      <td>243376.000000</td>\n",
       "      <td>7.621400e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>0.980942</td>\n",
       "      <td>0.019058</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24392</td>\n",
       "      <td>2074.0</td>\n",
       "      <td>951.0</td>\n",
       "      <td>4157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>255259.60</td>\n",
       "      <td>695.000000</td>\n",
       "      <td>53649.000000</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>2.493030e+06</td>\n",
       "      <td>178822.000000</td>\n",
       "      <td>1.016244e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14563</th>\n",
       "      <td>0.980921</td>\n",
       "      <td>0.019079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5077</td>\n",
       "      <td>2327.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>4724.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>-1028372.00</td>\n",
       "      <td>1610.000000</td>\n",
       "      <td>137286.000000</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>9.165400e+05</td>\n",
       "      <td>410879.000000</td>\n",
       "      <td>3.460296e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6336</th>\n",
       "      <td>0.980895</td>\n",
       "      <td>0.019105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7549</td>\n",
       "      <td>440.0</td>\n",
       "      <td>3493.0</td>\n",
       "      <td>1554.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-1050248.00</td>\n",
       "      <td>910.000000</td>\n",
       "      <td>17523.000000</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>4.324885e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.964880e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23206</th>\n",
       "      <td>0.980718</td>\n",
       "      <td>0.019282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17842</td>\n",
       "      <td>689.0</td>\n",
       "      <td>1759.0</td>\n",
       "      <td>4435.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-293891.20</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>20709.000000</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>7.400000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.258200e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21342</th>\n",
       "      <td>0.980351</td>\n",
       "      <td>0.019649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29696</td>\n",
       "      <td>2477.0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>79187.60</td>\n",
       "      <td>565.000000</td>\n",
       "      <td>28341.154719</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.057660e+06</td>\n",
       "      <td>397530.000000</td>\n",
       "      <td>1.048512e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>270.791998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prob_not_churn  prob_churn  true_value  index  Var198  Var199  Var202  \\\n",
       "16909        0.985350    0.014650         1.0  16197    34.0   885.0  4208.0   \n",
       "3393         0.985323    0.014677         1.0  21104  2457.0   176.0  3340.0   \n",
       "14202        0.985077    0.014923         1.0  25391  2607.0   901.0   592.0   \n",
       "8407         0.984915    0.015085         1.0    542  1677.0  2540.0   778.0   \n",
       "17194        0.984629    0.015371         1.0  14826   640.0   910.0  3108.0   \n",
       "21679        0.983909    0.016091         1.0  17875  2457.0   172.0  1267.0   \n",
       "23935        0.983649    0.016351         1.0    903  1174.0   408.0  4647.0   \n",
       "9147         0.983372    0.016628         1.0  25115  2495.0  2803.0  1274.0   \n",
       "12978        0.983102    0.016898         1.0   7163   158.0  1156.0   289.0   \n",
       "12865        0.982560    0.017440         1.0  21292  1410.0   172.0   352.0   \n",
       "21257        0.982116    0.017884         1.0  28501  2327.0  1331.0  2014.0   \n",
       "62           0.981487    0.018513         1.0  17306  2327.0  2791.0  3886.0   \n",
       "7005         0.981380    0.018620         1.0  13492  2324.0  3506.0  4537.0   \n",
       "7407         0.981240    0.018760         1.0  17323   950.0  2602.0  4466.0   \n",
       "3957         0.981105    0.018895         1.0  18550    82.0   749.0  2810.0   \n",
       "9166         0.980942    0.019058         1.0  24392  2074.0   951.0  4157.0   \n",
       "14563        0.980921    0.019079         1.0   5077  2327.0   729.0  4724.0   \n",
       "6336         0.980895    0.019105         1.0   7549   440.0  3493.0  1554.0   \n",
       "23206        0.980718    0.019282         1.0  17842   689.0  1759.0  4435.0   \n",
       "21342        0.980351    0.019649         1.0  29696  2477.0   885.0   489.0   \n",
       "\n",
       "       Var205  Var206  Var207  ...      Var109      Var113       Var119  \\\n",
       "16909     1.0     6.0     8.0  ...   56.000000    53292.40   555.000000   \n",
       "3393      1.0     7.0     4.0  ...  184.000000   144296.00   925.000000   \n",
       "14202     1.0    21.0     5.0  ...   32.000000   176128.00   535.000000   \n",
       "8407      0.0    18.0    10.0  ...   32.000000    84054.00   645.000000   \n",
       "17194     3.0    15.0    10.0  ...   16.000000   -90806.40   135.000000   \n",
       "21679     1.0    19.0     4.0  ...   88.000000    18085.16  1230.000000   \n",
       "23935     1.0     6.0     5.0  ...   56.000000 -1809956.00   915.000000   \n",
       "9147      1.0    16.0    10.0  ...   61.041707 -1859604.00   912.161453   \n",
       "12978     1.0    21.0    10.0  ...   32.000000   127444.40   515.000000   \n",
       "12865     1.0    18.0     9.0  ...   32.000000 -1849496.00   460.000000   \n",
       "21257     0.0    11.0     4.0  ...    8.000000    51163.20   575.000000   \n",
       "62        1.0    17.0     4.0  ...   32.000000   287952.40   530.000000   \n",
       "7005      0.0    21.0     5.0  ...   40.000000   216084.00   535.000000   \n",
       "7407      1.0    21.0     8.0  ...   40.000000   107190.80   560.000000   \n",
       "3957      1.0     6.0    10.0  ...   48.000000  -198978.40  1275.000000   \n",
       "9166      0.0     6.0    10.0  ...   48.000000   255259.60   695.000000   \n",
       "14563     1.0     9.0     4.0  ...  152.000000 -1028372.00  1610.000000   \n",
       "6336      1.0     6.0    10.0  ...   40.000000 -1050248.00   910.000000   \n",
       "23206     1.0     3.0     5.0  ...    0.000000  -293891.20   325.000000   \n",
       "21342     1.0     6.0    10.0  ...   48.000000    79187.60   565.000000   \n",
       "\n",
       "              Var125  Var126        Var133         Var149        Var153  \\\n",
       "16909   47781.000000   -30.0  1.963650e+06  604800.000000  1.048888e+07   \n",
       "3393   157401.000000   -30.0  1.030220e+06   77959.000000  1.055156e+07   \n",
       "14202   26172.000000   -28.0  5.148350e+06       0.000000  1.073044e+07   \n",
       "8407    43110.000000   -30.0  2.367195e+06       0.000000  1.009420e+07   \n",
       "17194   85437.000000   -30.0  5.999000e+04  126784.000000  1.360200e+05   \n",
       "21679   15651.000000   -30.0  9.028350e+05  358225.000000  1.810816e+06   \n",
       "23935    8883.000000   -20.0  1.785250e+05   74368.000000  4.200360e+05   \n",
       "9147    28341.154719   -20.0  2.274060e+06  298509.021966  6.158730e+06   \n",
       "12978     117.000000   -30.0  8.164550e+06       0.000000  1.052960e+07   \n",
       "12865   15075.000000   -18.0  1.419900e+05   10010.000000  2.112720e+05   \n",
       "21257   18621.000000   -26.0  1.436975e+06       0.000000  9.593880e+06   \n",
       "62      49032.000000   -24.0  6.894200e+06       0.000000  9.979160e+06   \n",
       "7005    28341.154719   -28.0  8.190800e+06   48629.000000  1.041268e+07   \n",
       "7407    24165.000000   -26.0  2.173735e+06       0.000000  1.021912e+07   \n",
       "3957    87246.000000   -28.0  2.867490e+06  243376.000000  7.621400e+06   \n",
       "9166    53649.000000   -28.0  2.493030e+06  178822.000000  1.016244e+07   \n",
       "14563  137286.000000   -24.0  9.165400e+05  410879.000000  3.460296e+06   \n",
       "6336    17523.000000   -20.0  4.324885e+06       0.000000  4.964880e+06   \n",
       "23206   20709.000000   -22.0  7.400000e+02       0.000000  7.258200e+06   \n",
       "21342   28341.154719   -30.0  1.057660e+06  397530.000000  1.048512e+07   \n",
       "\n",
       "         Var173      Var189  \n",
       "16909  0.000000  276.000000  \n",
       "3393   0.000000  270.791998  \n",
       "14202  0.000000  270.791998  \n",
       "8407   0.000000  270.791998  \n",
       "17194  0.000000  270.791998  \n",
       "21679  0.000000  270.791998  \n",
       "23935  0.000000  270.791998  \n",
       "9147   0.006368  270.791998  \n",
       "12978  0.000000  432.000000  \n",
       "12865  0.000000  270.791998  \n",
       "21257  0.000000  270.791998  \n",
       "62     0.000000  270.791998  \n",
       "7005   0.000000  270.791998  \n",
       "7407   0.000000  330.000000  \n",
       "3957   0.000000  270.791998  \n",
       "9166   0.000000  270.791998  \n",
       "14563  0.000000  270.791998  \n",
       "6336   0.000000  270.791998  \n",
       "23206  0.000000  270.791998  \n",
       "21342  0.000000  270.791998  \n",
       "\n",
       "[20 rows x 40 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Объекты, отнесенные не верно отнесенные к классу \"не отток\" с вероятностью больше 98%')\n",
    "objects[(objects.true_value == 1)&(objects.prob_not_churn >= 0.98)].sort_values(by='prob_not_churn', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объекты, отнесенные не верно отнесенные к классу \"отток\" с вероятностью больше 50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prob_not_churn</th>\n",
       "      <th>prob_churn</th>\n",
       "      <th>true_value</th>\n",
       "      <th>index</th>\n",
       "      <th>Var198</th>\n",
       "      <th>Var199</th>\n",
       "      <th>Var202</th>\n",
       "      <th>Var205</th>\n",
       "      <th>Var206</th>\n",
       "      <th>Var207</th>\n",
       "      <th>...</th>\n",
       "      <th>Var109</th>\n",
       "      <th>Var113</th>\n",
       "      <th>Var119</th>\n",
       "      <th>Var125</th>\n",
       "      <th>Var126</th>\n",
       "      <th>Var133</th>\n",
       "      <th>Var149</th>\n",
       "      <th>Var153</th>\n",
       "      <th>Var173</th>\n",
       "      <th>Var189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20164</th>\n",
       "      <td>0.407796</td>\n",
       "      <td>0.592204</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1555</td>\n",
       "      <td>2579.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>4185.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.041707</td>\n",
       "      <td>585260.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>298509.021966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prob_not_churn  prob_churn  true_value  index  Var198  Var199  Var202  \\\n",
       "20164        0.407796    0.592204        -1.0   1555  2579.0   524.0  4185.0   \n",
       "\n",
       "       Var205  Var206  Var207  ...     Var109    Var113  Var119  Var125  \\\n",
       "20164     1.0    21.0    10.0  ...  61.041707  585260.0    20.0     0.0   \n",
       "\n",
       "       Var126  Var133         Var149  Var153  Var173  Var189  \n",
       "20164     4.0     0.0  298509.021966     0.0     0.0   228.0  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Объекты, отнесенные не верно отнесенные к классу \"отток\" с вероятностью больше 50%')\n",
    "objects[(objects.true_value == -1)&(objects.prob_churn >= 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Видно, что модель все еще плохо предсказывает отток, а высокое качество на кросс валиадции достигается за счет несбалансированности выборки. Верно предсказан класс 1 только для 20 объектов из 1773. Однако классификатор редко ошибается в случае неверного отнесения к классу отток (всего одно наблюдение). Закономерности между объектами, которые предсказываются неверно, не видно.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Лучшая модель на основе всех преобразований была построена в п.6. Для это было сделано следующее:*\n",
    "    1. Пропуски в категориальных данных заменены на \"missing_value\", пропуски в числовых заменены на средние значения по\n",
    "    столбцам;\n",
    "    2. Категориальные призанки закодированы с помощью OrdonalEncoder;\n",
    "    3. C помощью метода SelectFromModel определены наиболее значимые признаки;\n",
    "    4. Обучена модель градиентного бустинга с параметрами {'learning_rate': 0.05,\n",
    "                                                           'loss': 'exponential',\n",
    "                                                           'max_depth': 3,\n",
    "                                                           'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*При попытке протестировать полученную модель на данных kaggle качетсво сильно упало по сравнению с baseline моделью, поэтому лучше использовать все данные и переменные, не исключая их. Также улучшить модель скорее всего можно применяяя другие модели, которые лучше всего работают с разреженными, неполными данными, а также с данными в которых классы несбалансированные. Возможно, лучше применять oversampling вместо undersampling*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
